{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Data 598 (Winter 2022): HW5 </h1> </center> \n",
    "    <center> University of Washington </center>\n",
    "    \n",
    "Please fill out all the `TODO`s in the notebook below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a differentiable module\n",
    "\n",
    "Consider the soft-thresholding function $f_T: \\mathbb{R} \\to \\mathbb{R}$ defined for any $T > 0$ as \n",
    "$$\n",
    "    f_T(y) = \n",
    "    \\begin{cases} \n",
    "        0, & \\text{ if } -T \\le y \\le T \\,, \\\\\n",
    "        y - T, & \\text{ if } y > T \\,, \\\\\n",
    "        y + T, & \\text{ if } y < T \\,.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "See the image below for $T=3$. \n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/32c265c127e6985e365b93158123655e13768ea4/6-Figure2-1.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Write a function to compute which takes in as arguments $y, T$ and returns the soft-thresholding $f_T(y)$.\n",
    "    Plot this function with $T = 3.14$ in the range $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example of PyTorch Scalar\n",
    "x = torch.tensor(3.14159, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def softt(y, T):\n",
    "    \"\"\" `y` is a torch.tensor (i.e., PyTorch's scalar type; same as above), \n",
    "        `T` is a regular Python number (float or int).\n",
    "        return type: torch.tensor\n",
    "    \"\"\"\n",
    "    # TODO: your code here\n",
    "    # HINT: if you write a program with branches, make sure that the output type is always a torch.tensor\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Write a function which computes the derivate $f_T'(y)$ of the soft-thresholding function w.r.t. $y$, as returned by PyTorch. Plot this for $T=3.14$ in the range $[-10, 10]$. \n",
    "\n",
    "**Hint 1**: If you coded up `softt` using branches, you might encounter a situation where the output does not depend on the input. In this case, you will have to appropriately set the `allow_unused` flag. \n",
    "\n",
    "**Hint 2**: When PyTorch returns a derivative of `None`, it actually stands for `0`. If your derivative returns a `None`, you will have to handle this appropriately when plotting the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softt_derivative(y, T): \n",
    "    # TODO: your code here\n",
    "    # Call torch.autograd.grad to compute the derivative\n",
    "\n",
    "# Test your code\n",
    "a = torch.tensor(1.20, requires_grad=True)\n",
    "print(softt_derivative(a, 3.14))\n",
    "\n",
    "# Plot\n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** We will now code a differentiable module using `torch.nn.Module`. \n",
    "\n",
    "First, let us extend the definition of \n",
    "the soft-thresholding $f_T$ to vectors by applying the soft-thresholding operation component-wise. \n",
    "\n",
    "Now write a differentiable module which implements the transformation $g_{T}(x, A; M)$ given by \n",
    "$$\n",
    "    g_{T}(x, A; M) = M^{-1} \\, f_T\\big( (A^{-1} A)^n  Mx \\big) \\,,\n",
    "$$\n",
    "\n",
    "where $n=10000$ is given. Note that we can simplify $ A^{-1} A = I$ to obtain the same result. However, our chain (going right to left) contains the repetitive and unnecessary computation of multiplying $Mx$ by $A$ and then immediately undoing it by multiplying by $A^{-1}$. \n",
    "\n",
    "Note that $x \\in \\mathbb{R}^d$ is a vector, $A \\in \\mathbb{R}^{d\\times d}$ is an invertible matrix and the output is a vector is a vector in $\\mathbb{R}^d$.\n",
    "\n",
    "Here, $M \\in \\mathbb{R}^{d \\times d}$, a symmetric matrix, is a *parameter* of the module. (Recall: parameters maintain state of the module; register a parameter in `torch.nn.Module` by using the `torch.nn.Parameter` wrapper).\n",
    "\n",
    "Supply $T > 0$ and and initial value $M_0 \\in \\mathbb{R}^{d \\times d}$ symmetric to the constructor, while the `forward` method only accepts $x \\in \\mathbb{R}^d$ and $A \\in \\mathbb{R}^{d \\times d}$ as inputs. \n",
    "\n",
    "You may use the function `create_symmetric_invertible_matrix` to initialize this matrix `M` in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WastefulMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Initialize the module with $T = 3.14$ and $M_0$ using the function `create_symmetric_invertible_matrix` with `seed=0`.  \n",
    "Use `dimension=5`. Pass in the following vector `x` and matrix `A` defined below and compute $g_T(x, A;M_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_symmetric_invertible_matrix(dimension, seed=0):\n",
    "    # return symmetric invertible square matrix of size `dimension` x `dimension`\n",
    "    rng = np.random.RandomState(dimension + seed)\n",
    "    factor = rng.randn(dimension, dimension)  # use dtype double\n",
    "    return 1e-6 * torch.eye(dimension) + torch.from_numpy(np.matmul(factor, factor.T))\n",
    "\n",
    "dimension = 5\n",
    "x = torch.DoubleTensor([0.1, 5, -2.3, -1, -2]).requires_grad_(True)  # use dtype double\n",
    "A = create_symmetric_invertible_matrix(dimension, seed=10).requires_grad_(True)\n",
    "print('x:', x)\n",
    "# TODO: your code here using `WastefulMatmulSofttMatmulinv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** For the same vector `x` as defined above, compute and print out the gradient of $\\varphi_T(x, A; M) = \\|x - g_T(x, A; M)\\|_2^2$\n",
    "with respect to $x$, $A$, and $M$ using automatic differentiation. Use $T=3.14$ again.\n",
    "\n",
    "Time the computation of the gradient using Python's `time` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F)** Repeat parts C-E above but with an efficient version of `WastefulMatmulSofttMatmulinv` that utilizes the simplification $A^{-1} A = I$ to return the exact same output. \n",
    "\n",
    "Note how much time the computation of the gradient takes. Why do you observe the discrepancy in the run times? Do you observe any discrepancy in the gradients? If yes, why? \n",
    "\n",
    "**Hint**: Set the flag `allow_unused=True` in the call to `torch.auto.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here with EfficientMatmulSofttMatmulinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the difference of gradients from EfficientMatmulSofttMatmulinv and WastefulMatmulSofttMatmulinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
