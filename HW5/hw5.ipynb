{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Data 598 (Winter 2022): HW5 </h1> </center> \n",
    "    <center> University of Washington </center>\n",
    "    \n",
    "Please fill out all the `TODO`s in the notebook below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a differentiable module\n",
    "\n",
    "Consider the soft-thresholding function $f_T: \\mathbb{R} \\to \\mathbb{R}$ defined for any $T > 0$ as \n",
    "$$\n",
    "    f_T(y) = \n",
    "    \\begin{cases} \n",
    "        0, & \\text{ if } -T \\le y \\le T \\,, \\\\\n",
    "        y - T, & \\text{ if } y > T \\,, \\\\\n",
    "        y + T, & \\text{ if } y < T \\,.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "See the image below for $T=3$. \n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/32c265c127e6985e365b93158123655e13768ea4/6-Figure2-1.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Write a function to compute which takes in as arguments $y, T$ and returns the soft-thresholding $f_T(y)$.\n",
    "    Plot this function with $T = 3.14$ in the range $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example of PyTorch Scalar\n",
    "x = torch.tensor(3.14159, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def softt(y, T):\n",
    "    \"\"\" `y` is a torch.tensor (i.e., PyTorch's scalar type; same as above), \n",
    "        `T` is a regular Python number (float or int).\n",
    "        return type: torch.tensor\n",
    "    \"\"\"\n",
    "    if y > T:\n",
    "        return torch.add(y, -T)\n",
    "    elif y < -T:\n",
    "        return torch.add(y, T)\n",
    "    else:\n",
    "        return torch.zeros_like(y, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Write a function which computes the derivate $f_T'(y)$ of the soft-thresholding function w.r.t. $y$, as returned by PyTorch. Plot this for $T=3.14$ in the range $[-10, 10]$. \n",
    "\n",
    "**Hint 1**: If you coded up `softt` using branches, you might encounter a situation where the output does not depend on the input. In this case, you will have to appropriately set the `allow_unused` flag. \n",
    "\n",
    "**Hint 2**: When PyTorch returns a derivative of `None`, it actually stands for `0`. If your derivative returns a `None`, you will have to handle this appropriately when plotting the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Soft thresholding derivative with $T=3.14$')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softt_derivative(y, T): \n",
    "    # TODO: your code here\n",
    "    # Call torch.autograd.grad to compute the derivative\n",
    "    f = softt(y, T)\n",
    "    grad = torch.autograd.grad(outputs=f, inputs=y, allow_unused=True)\n",
    "    return grad\n",
    "\n",
    "# Test your code\n",
    "a = torch.tensor(1.2, requires_grad=True)\n",
    "print(softt_derivative(a, 1.2))\n",
    "\n",
    "# Plot\n",
    "# TODO: your code here\n",
    "ys = torch.linspace(-10, 10, 1000, requires_grad=True)\n",
    "grads = [softt_derivative(y, 3.14)[0] if softt_derivative(y, 3.14)[0] is not None else 0 for y in ys]\n",
    "plt.plot(ys.detach().numpy(), np.asarray(grads))\n",
    "plt.title('Soft thresholding derivative with $T=3.14$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** We will now code a differentiable module using `torch.nn.Module`. \n",
    "\n",
    "First, let us extend the definition of \n",
    "the soft-thresholding $f_T$ to vectors by applying the soft-thresholding operation component-wise. \n",
    "\n",
    "Now write a differentiable module which implements the transformation $g_{T}(x, A; M)$ given by \n",
    "$$\n",
    "    g_{T}(x, A; M) = M^{-1} \\, f_T\\big( (A^{-1} A)^n  Mx \\big) \\,,\n",
    "$$\n",
    "\n",
    "where $n=10000$ is given. Note that we can simplify $ A^{-1} A = I$ to obtain the same result. However, our chain (going right to left) contains the repetitive and unnecessary computation of multiplying $Mx$ by $A$ and then immediately undoing it by multiplying by $A^{-1}$. \n",
    "\n",
    "Note that $x \\in \\mathbb{R}^d$ is a vector, $A \\in \\mathbb{R}^{d\\times d}$ is an invertible matrix and the output is a vector is a vector in $\\mathbb{R}^d$.\n",
    "\n",
    "Here, $M \\in \\mathbb{R}^{d \\times d}$, a symmetric matrix, is a *parameter* of the module. (Recall: parameters maintain state of the module; register a parameter in `torch.nn.Module` by using the `torch.nn.Parameter` wrapper).\n",
    "\n",
    "Supply $T > 0$ and and initial value $M_0 \\in \\mathbb{R}^{d \\times d}$ symmetric to the constructor, while the `forward` method only accepts $x \\in \\mathbb{R}^d$ as an input. \n",
    "\n",
    "You may use the function `create_symmetric_invertible_matrix` to initialize this matrix `M` in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WastefulMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here\n",
    "    def __init__(self, M, T, n):\n",
    "        super().__init__()\n",
    "        self.M = torch.nn.Parameter(M)\n",
    "        self.T = T\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x, A):\n",
    "        res = torch.matmul(self.M, x)\n",
    "        for _ in range(self.n):\n",
    "            res = torch.matmul(A, res)\n",
    "            res = torch.matmul(torch.linalg.inv(A), res)\n",
    "        f_T = torch.tensor([softt(y, self.T) for y in res])\n",
    "        g_T = torch.matmul(torch.linalg.inv(self.M), f_T)\n",
    "        return g_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Initialize the module with $T = 3.14$ and $M_0$ using the function `create_symmetric_invertible_matrix` with `seed=0`.  \n",
    "Use `dimension=5`. Pass in the following vector `x` and matrix `A` defined below and compute $g_T(x, A;M_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([ 0.1000,  5.0000, -2.3000, -1.0000, -2.0000], dtype=torch.float64,\n",
      "       requires_grad=True)\n",
      "g_T(x, A; M_0) = tensor([-0.6328,  6.8930, -0.5180,  1.2438, -0.6816], dtype=torch.float64,\n",
      "       grad_fn=<MvBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def create_symmetric_invertible_matrix(dimension, seed=0):\n",
    "    # return symmetric invertible square matrix of size `dimension` x `dimension`\n",
    "    rng = np.random.RandomState(dimension + seed)\n",
    "    factor = rng.randn(dimension, dimension)  # use dtype double\n",
    "    return 1e-6 * torch.eye(dimension) + torch.from_numpy(np.matmul(factor, factor.T))\n",
    "\n",
    "dimension = 5\n",
    "x = torch.DoubleTensor([0.1, 5, -2.3, -1, -2]).requires_grad_(True)  # use dtype double\n",
    "A = create_symmetric_invertible_matrix(dimension, seed=10).requires_grad_(True)\n",
    "print('x:', x)\n",
    "# TODO: your code here using `WastefulMatmulSofttMatmulinv`\n",
    "M = create_symmetric_invertible_matrix(dimension, seed=0)\n",
    "wasteful = WastefulMatmulSofttMatmulinv(M, 3.14, 10000)\n",
    "g = wasteful(x, A)\n",
    "print(f'g_T(x, A; M_0) = {g}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** For the same vector `x` as defined above, compute and print out the gradient of $\\varphi_T(x, A; M) = \\|x - g_T(x, A; M)\\|_2^2$\n",
    "with respect to $x$, $A$, and $M$ using automatic differentiation. Use $T=3.14$ again.\n",
    "\n",
    "Time the computation of the gradient using Python's `time` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.38019371032714844\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "# TODO: your code here\n",
    "start = time.time()\n",
    "wasteful = WastefulMatmulSofttMatmulinv(M, 3.14, 10000)\n",
    "f_wasteful = wasteful(x, A)\n",
    "norm_wasteful = torch.linalg.norm(x - f_wasteful)**2\n",
    "grad_wasteful = torch.autograd.grad(outputs=norm_wasteful, inputs=[x, A], allow_unused=True)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F)** Repeat parts C-E above but with an efficient version of `WastefulMatmulSofttMatmulinv` that utilizes the simplification $A^{-1} A = I$ to return the exact same output. \n",
    "\n",
    "Note how much time the computation of the gradient takes. Why do you observe the discrepancy in the run times? Do you observe any discrepancy in the gradients? If yes, why? \n",
    "\n",
    "**Hint**: Set the flag `allow_unused=True` in the call to `torch.auto.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here\n",
    "    def __init__(self, M, T, n):\n",
    "        super().__init__()\n",
    "        self.M = torch.nn.Parameter(M)\n",
    "        self.T = T\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x, A):\n",
    "        res = torch.matmul(self.M, x)\n",
    "        f_T = torch.tensor([softt(y, self.T) for y in res])\n",
    "        g_T = torch.matmul(torch.linalg.inv(self.M), f_T)\n",
    "        return g_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 0.0510401725769043\n"
     ]
    }
   ],
   "source": [
    "# TODO: your code here with EfficientMatmulSofttMatmulinv\n",
    "import time\n",
    "start = time.time()\n",
    "efficient = EfficientMatmulSofttMatmulinv(M, 3.14, 10000)\n",
    "f_efficient = efficient(x, A)\n",
    "norm_efficient = torch.linalg.norm(x - f_efficient)**2\n",
    "grad_efficient = torch.autograd.grad(outputs=norm_efficient, inputs=[x, A], allow_unused=True)\n",
    "end = time.time()\n",
    "\n",
    "print(f'Total time: {end - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a discrepancy in runtimes because in the efficient version of the module, we are forgoing the n number of reptitions of calculating $I$ via $A^TA$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "diff(): argument 'n' (position 2) must be int, not Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_93/394297980.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: check the difference of gradients from EfficientMatmulSofttMatmulinv and WastefulMatmulSofttMatmulinv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_efficient\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_wasteful\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: diff(): argument 'n' (position 2) must be int, not Tensor"
     ]
    }
   ],
   "source": [
    "# TODO: check the difference of gradients from EfficientMatmulSofttMatmulinv and WastefulMatmulSofttMatmulinv\n",
    "torch.diff(grad_efficient[0], grad_wasteful[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0632e-11, -9.3076e-11, -5.1182e-11, -7.7318e-11, -3.2657e-11],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_efficient[0] - grad_wasteful[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a big difference between the gradients for the efficient and wasteful modules."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
