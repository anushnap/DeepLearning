{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  <h1> Data 598 (Winter 2022): HW5 </h1> </center> \n",
    "    <center> University of Washington </center>\n",
    "    \n",
    "Please fill out all the `TODO`s in the notebook below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding up a differentiable module\n",
    "\n",
    "Consider the soft-thresholding function $f_T: \\mathbb{R} \\to \\mathbb{R}$ defined for any $T > 0$ as \n",
    "$$\n",
    "    f_T(y) = \n",
    "    \\begin{cases} \n",
    "        0, & \\text{ if } -T \\le y \\le T \\,, \\\\\n",
    "        y - T, & \\text{ if } y > T \\,, \\\\\n",
    "        y + T, & \\text{ if } y < T \\,.\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "See the image below for $T=3$. \n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/32c265c127e6985e365b93158123655e13768ea4/6-Figure2-1.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A)** Write a function to compute which takes in as arguments $y, T$ and returns the soft-thresholding $f_T(y)$.\n",
    "    Plot this function with $T = 3.14$ in the range $[-10, 10]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1416, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example of PyTorch Scalar\n",
    "x = torch.tensor(3.14159, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def softt(y, T):\n",
    "    \"\"\" `y` is a torch.tensor (i.e., PyTorch's scalar type; same as above), \n",
    "        `T` is a regular Python number (float or int).\n",
    "        return type: torch.tensor\n",
    "    \"\"\"\n",
    "#     if y > T:\n",
    "#         return torch.add(y, -T)\n",
    "#     elif y < -T:\n",
    "#         return torch.add(y, T)\n",
    "#     else:\n",
    "#         return torch.zeros_like(y, requires_grad=True)\n",
    "    return torch.where(\n",
    "        y > T, torch.add(y, -T), # if y > T then y - T\n",
    "        torch.where(y < -T, torch.add(y, T), # elif y < -T then y + T\n",
    "                    torch.zeros_like(torch.tensor(0.), requires_grad=True) # else 0\n",
    "                   )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B)** Write a function which computes the derivate $f_T'(y)$ of the soft-thresholding function w.r.t. $y$, as returned by PyTorch. Plot this for $T=3.14$ in the range $[-10, 10]$. \n",
    "\n",
    "**Hint 1**: If you coded up `softt` using branches, you might encounter a situation where the output does not depend on the input. In this case, you will have to appropriately set the `allow_unused` flag. \n",
    "\n",
    "**Hint 2**: When PyTorch returns a derivative of `None`, it actually stands for `0`. If your derivative returns a `None`, you will have to handle this appropriately when plotting the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.),)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Soft thresholding derivative with $T=3.14$')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def softt_derivative(y, T): \n",
    "    # TODO: your code here\n",
    "    # Call torch.autograd.grad to compute the derivative\n",
    "    f = softt(y, T)\n",
    "    grad = torch.autograd.grad(outputs=f, inputs=y, allow_unused=True)\n",
    "    return grad\n",
    "\n",
    "# Test your code\n",
    "a = torch.tensor(1.2, requires_grad=True)\n",
    "print(softt_derivative(a, 1.2))\n",
    "\n",
    "# Plot\n",
    "# TODO: your code here\n",
    "ys = torch.linspace(-10, 10, 1000, requires_grad=True)\n",
    "grads = [softt_derivative(y, 3.14) for y in ys]\n",
    "plt.plot(ys.detach().numpy(), np.asarray(grads))\n",
    "plt.title('Soft thresholding derivative with $T=3.14$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-6.8600, -6.8400, -6.8200, -6.7999, -6.7799, -6.7599, -6.7399, -6.7199,\n",
       "        -6.6998, -6.6798, -6.6598, -6.6398, -6.6198, -6.5997, -6.5797, -6.5597,\n",
       "        -6.5397, -6.5197, -6.4996, -6.4796, -6.4596, -6.4396, -6.4196, -6.3995,\n",
       "        -6.3795, -6.3595, -6.3395, -6.3195, -6.2994, -6.2794, -6.2594, -6.2394,\n",
       "        -6.2194, -6.1993, -6.1793, -6.1593, -6.1393, -6.1193, -6.0992, -6.0792,\n",
       "        -6.0592, -6.0392, -6.0192, -5.9991, -5.9791, -5.9591, -5.9391, -5.9191,\n",
       "        -5.8990, -5.8790, -5.8590, -5.8390, -5.8190, -5.7989, -5.7789, -5.7589,\n",
       "        -5.7389, -5.7189, -5.6988, -5.6788, -5.6588, -5.6388, -5.6188, -5.5987,\n",
       "        -5.5787, -5.5587, -5.5387, -5.5187, -5.4986, -5.4786, -5.4586, -5.4386,\n",
       "        -5.4186, -5.3985, -5.3785, -5.3585, -5.3385, -5.3185, -5.2984, -5.2784,\n",
       "        -5.2584, -5.2384, -5.2184, -5.1983, -5.1783, -5.1583, -5.1383, -5.1183,\n",
       "        -5.0982, -5.0782, -5.0582, -5.0382, -5.0182, -4.9981, -4.9781, -4.9581,\n",
       "        -4.9381, -4.9181, -4.8980, -4.8780, -4.8580, -4.8380, -4.8180, -4.7979,\n",
       "        -4.7779, -4.7579, -4.7379, -4.7179, -4.6978, -4.6778, -4.6578, -4.6378,\n",
       "        -4.6178, -4.5977, -4.5777, -4.5577, -4.5377, -4.5177, -4.4976, -4.4776,\n",
       "        -4.4576, -4.4376, -4.4176, -4.3975, -4.3775, -4.3575, -4.3375, -4.3175,\n",
       "        -4.2974, -4.2774, -4.2574, -4.2374, -4.2174, -4.1973, -4.1773, -4.1573,\n",
       "        -4.1373, -4.1173, -4.0972, -4.0772, -4.0572, -4.0372, -4.0172, -3.9971,\n",
       "        -3.9771, -3.9571, -3.9371, -3.9171, -3.8970, -3.8770, -3.8570, -3.8370,\n",
       "        -3.8170, -3.7969, -3.7769, -3.7569, -3.7369, -3.7169, -3.6968, -3.6768,\n",
       "        -3.6568, -3.6368, -3.6168, -3.5967, -3.5767, -3.5567, -3.5367, -3.5167,\n",
       "        -3.4966, -3.4766, -3.4566, -3.4366, -3.4166, -3.3965, -3.3765, -3.3565,\n",
       "        -3.3365, -3.3165, -3.2964, -3.2764, -3.2564, -3.2364, -3.2164, -3.1963,\n",
       "        -3.1763, -3.1563, -3.1363, -3.1163, -3.0962, -3.0762, -3.0562, -3.0362,\n",
       "        -3.0162, -2.9961, -2.9761, -2.9561, -2.9361, -2.9161, -2.8960, -2.8760,\n",
       "        -2.8560, -2.8360, -2.8160, -2.7959, -2.7759, -2.7559, -2.7359, -2.7159,\n",
       "        -2.6958, -2.6758, -2.6558, -2.6358, -2.6158, -2.5957, -2.5757, -2.5557,\n",
       "        -2.5357, -2.5157, -2.4956, -2.4756, -2.4556, -2.4356, -2.4156, -2.3955,\n",
       "        -2.3755, -2.3555, -2.3355, -2.3155, -2.2954, -2.2754, -2.2554, -2.2354,\n",
       "        -2.2154, -2.1953, -2.1753, -2.1553, -2.1353, -2.1153, -2.0952, -2.0752,\n",
       "        -2.0552, -2.0352, -2.0152, -1.9951, -1.9751, -1.9551, -1.9351, -1.9151,\n",
       "        -1.8950, -1.8750, -1.8550, -1.8350, -1.8150, -1.7949, -1.7749, -1.7549,\n",
       "        -1.7349, -1.7149, -1.6948, -1.6748, -1.6548, -1.6348, -1.6148, -1.5947,\n",
       "        -1.5747, -1.5547, -1.5347, -1.5147, -1.4946, -1.4746, -1.4546, -1.4346,\n",
       "        -1.4146, -1.3945, -1.3745, -1.3545, -1.3345, -1.3145, -1.2944, -1.2744,\n",
       "        -1.2544, -1.2344, -1.2144, -1.1943, -1.1743, -1.1543, -1.1343, -1.1143,\n",
       "        -1.0942, -1.0742, -1.0542, -1.0342, -1.0142, -0.9941, -0.9741, -0.9541,\n",
       "        -0.9341, -0.9141, -0.8940, -0.8740, -0.8540, -0.8340, -0.8140, -0.7939,\n",
       "        -0.7739, -0.7539, -0.7339, -0.7139, -0.6938, -0.6738, -0.6538, -0.6338,\n",
       "        -0.6138, -0.5937, -0.5737, -0.5537, -0.5337, -0.5137, -0.4936, -0.4736,\n",
       "        -0.4536, -0.4336, -0.4136, -0.3935, -0.3735, -0.3535, -0.3335, -0.3135,\n",
       "        -0.2934, -0.2734, -0.2534, -0.2334, -0.2134, -0.1933, -0.1733, -0.1533,\n",
       "        -0.1333, -0.1133, -0.0932, -0.0732, -0.0532, -0.0332, -0.0132,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0132,  0.0332,  0.0532,  0.0732,  0.0932,  0.1133,  0.1333,\n",
       "         0.1533,  0.1733,  0.1933,  0.2134,  0.2334,  0.2534,  0.2734,  0.2934,\n",
       "         0.3135,  0.3335,  0.3535,  0.3735,  0.3935,  0.4136,  0.4336,  0.4536,\n",
       "         0.4736,  0.4936,  0.5137,  0.5337,  0.5537,  0.5737,  0.5937,  0.6138,\n",
       "         0.6338,  0.6538,  0.6738,  0.6938,  0.7139,  0.7339,  0.7539,  0.7739,\n",
       "         0.7939,  0.8140,  0.8340,  0.8540,  0.8740,  0.8940,  0.9141,  0.9341,\n",
       "         0.9541,  0.9741,  0.9941,  1.0142,  1.0342,  1.0542,  1.0742,  1.0942,\n",
       "         1.1143,  1.1343,  1.1543,  1.1743,  1.1943,  1.2144,  1.2344,  1.2544,\n",
       "         1.2744,  1.2944,  1.3145,  1.3345,  1.3545,  1.3745,  1.3945,  1.4146,\n",
       "         1.4346,  1.4546,  1.4746,  1.4946,  1.5147,  1.5347,  1.5547,  1.5747,\n",
       "         1.5947,  1.6148,  1.6348,  1.6548,  1.6748,  1.6948,  1.7149,  1.7349,\n",
       "         1.7549,  1.7749,  1.7949,  1.8150,  1.8350,  1.8550,  1.8750,  1.8950,\n",
       "         1.9151,  1.9351,  1.9551,  1.9751,  1.9951,  2.0152,  2.0352,  2.0552,\n",
       "         2.0752,  2.0952,  2.1153,  2.1353,  2.1553,  2.1753,  2.1953,  2.2154,\n",
       "         2.2354,  2.2554,  2.2754,  2.2954,  2.3155,  2.3355,  2.3555,  2.3755,\n",
       "         2.3955,  2.4156,  2.4356,  2.4556,  2.4756,  2.4956,  2.5157,  2.5357,\n",
       "         2.5557,  2.5757,  2.5957,  2.6158,  2.6358,  2.6558,  2.6758,  2.6958,\n",
       "         2.7159,  2.7359,  2.7559,  2.7759,  2.7959,  2.8160,  2.8360,  2.8560,\n",
       "         2.8760,  2.8960,  2.9161,  2.9361,  2.9561,  2.9761,  2.9961,  3.0162,\n",
       "         3.0362,  3.0562,  3.0762,  3.0962,  3.1163,  3.1363,  3.1563,  3.1763,\n",
       "         3.1963,  3.2164,  3.2364,  3.2564,  3.2764,  3.2964,  3.3165,  3.3365,\n",
       "         3.3565,  3.3765,  3.3965,  3.4166,  3.4366,  3.4566,  3.4766,  3.4966,\n",
       "         3.5167,  3.5367,  3.5567,  3.5767,  3.5967,  3.6168,  3.6368,  3.6568,\n",
       "         3.6768,  3.6968,  3.7169,  3.7369,  3.7569,  3.7769,  3.7969,  3.8170,\n",
       "         3.8370,  3.8570,  3.8770,  3.8970,  3.9171,  3.9371,  3.9571,  3.9771,\n",
       "         3.9971,  4.0172,  4.0372,  4.0572,  4.0772,  4.0972,  4.1173,  4.1373,\n",
       "         4.1573,  4.1773,  4.1973,  4.2174,  4.2374,  4.2574,  4.2774,  4.2974,\n",
       "         4.3175,  4.3375,  4.3575,  4.3775,  4.3975,  4.4176,  4.4376,  4.4576,\n",
       "         4.4776,  4.4976,  4.5177,  4.5377,  4.5577,  4.5777,  4.5977,  4.6178,\n",
       "         4.6378,  4.6578,  4.6778,  4.6978,  4.7179,  4.7379,  4.7579,  4.7779,\n",
       "         4.7979,  4.8180,  4.8380,  4.8580,  4.8780,  4.8980,  4.9181,  4.9381,\n",
       "         4.9581,  4.9781,  4.9981,  5.0182,  5.0382,  5.0582,  5.0782,  5.0982,\n",
       "         5.1183,  5.1383,  5.1583,  5.1783,  5.1983,  5.2184,  5.2384,  5.2584,\n",
       "         5.2784,  5.2984,  5.3185,  5.3385,  5.3585,  5.3785,  5.3985,  5.4186,\n",
       "         5.4386,  5.4586,  5.4786,  5.4986,  5.5187,  5.5387,  5.5587,  5.5787,\n",
       "         5.5987,  5.6188,  5.6388,  5.6588,  5.6788,  5.6988,  5.7189,  5.7389,\n",
       "         5.7589,  5.7789,  5.7989,  5.8190,  5.8390,  5.8590,  5.8790,  5.8990,\n",
       "         5.9191,  5.9391,  5.9591,  5.9791,  5.9991,  6.0192,  6.0392,  6.0592,\n",
       "         6.0792,  6.0992,  6.1193,  6.1393,  6.1593,  6.1793,  6.1993,  6.2194,\n",
       "         6.2394,  6.2594,  6.2794,  6.2994,  6.3195,  6.3395,  6.3595,  6.3795,\n",
       "         6.3995,  6.4196,  6.4396,  6.4596,  6.4796,  6.4996,  6.5197,  6.5397,\n",
       "         6.5597,  6.5797,  6.5997,  6.6198,  6.6398,  6.6598,  6.6798,  6.6998,\n",
       "         6.7199,  6.7399,  6.7599,  6.7799,  6.7999,  6.8200,  6.8400,  6.8600],\n",
       "       grad_fn=<SWhereBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softt(ys, 3.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C)** We will now code a differentiable module using `torch.nn.Module`. \n",
    "\n",
    "First, let us extend the definition of \n",
    "the soft-thresholding $f_T$ to vectors by applying the soft-thresholding operation component-wise. \n",
    "\n",
    "Now write a differentiable module which implements the transformation $g_{T}(x, A; M)$ given by \n",
    "$$\n",
    "    g_{T}(x, A; M) = M^{-1} \\, f_T\\big( (A^{-1} A)^n  Mx \\big) \\,,\n",
    "$$\n",
    "\n",
    "where $n=10000$ is given. Note that we can simplify $ A^{-1} A = I$ to obtain the same result. However, our chain (going right to left) contains the repetitive and unnecessary computation of multiplying $Mx$ by $A$ and then immediately undoing it by multiplying by $A^{-1}$. \n",
    "\n",
    "Note that $x \\in \\mathbb{R}^d$ is a vector, $A \\in \\mathbb{R}^{d\\times d}$ is an invertible matrix and the output is a vector is a vector in $\\mathbb{R}^d$.\n",
    "\n",
    "Here, $M \\in \\mathbb{R}^{d \\times d}$, a symmetric matrix, is a *parameter* of the module. (Recall: parameters maintain state of the module; register a parameter in `torch.nn.Module` by using the `torch.nn.Parameter` wrapper).\n",
    "\n",
    "Supply $T > 0$ and and initial value $M_0 \\in \\mathbb{R}^{d \\times d}$ symmetric to the constructor, while the `forward` method only accepts $x \\in \\mathbb{R}^d$ as an input. \n",
    "\n",
    "You may use the function `create_symmetric_invertible_matrix` to initialize this matrix `M` in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WastefulMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here\n",
    "    def __init__(self, M, T, n):\n",
    "        super.__init__()\n",
    "        self.M = torch.nn.parameter(M)\n",
    "        self.T = T\n",
    "        self.n = n\n",
    "    \n",
    "    def forward(self, x, A):\n",
    "        res = torch.matmul(self.M, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**D)** Initialize the module with $T = 3.14$ and $M_0$ using the function `create_symmetric_invertible_matrix` with `seed=0`.  \n",
    "Use `dimension=5`. Pass in the following vector `x` and matrix `A` defined below and compute $g_T(x, A;M_0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_symmetric_invertible_matrix(dimension, seed=0):\n",
    "    # return symmetric invertible square matrix of size `dimension` x `dimension`\n",
    "    rng = np.random.RandomState(dimension + seed)\n",
    "    factor = rng.randn(dimension, dimension)  # use dtype double\n",
    "    return 1e-6 * torch.eye(dimension) + torch.from_numpy(np.matmul(factor, factor.T))\n",
    "\n",
    "dimension = 5\n",
    "x = torch.DoubleTensor([0.1, 5, -2.3, -1, -2]).requires_grad_(True)  # use dtype double\n",
    "A = create_symmetric_invertible_matrix(dimension, seed=10).requires_grad_(True)\n",
    "print('x:', x)\n",
    "# TODO: your code here using `WastefulMatmulSofttMatmulinv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E)** For the same vector `x` as defined above, compute and print out the gradient of $\\varphi_T(x, A; M) = \\|x - g_T(x, A; M)\\|_2^2$\n",
    "with respect to $x$, $A$, and $M$ using automatic differentiation. Use $T=3.14$ again.\n",
    "\n",
    "Time the computation of the gradient using Python's `time` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F)** Repeat parts C-E above but with an efficient version of `WastefulMatmulSofttMatmulinv` that utilizes the simplification $A^{-1} A = I$ to return the exact same output. \n",
    "\n",
    "Note how much time the computation of the gradient takes. Why do you observe the discrepancy in the run times? Do you observe any discrepancy in the gradients? If yes, why? \n",
    "\n",
    "**Hint**: Set the flag `allow_unused=True` in the call to `torch.auto.grad`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientMatmulSofttMatmulinv(torch.nn.Module):\n",
    "    #### TODO: your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here with EfficientMatmulSofttMatmulinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check the difference of gradients from EfficientMatmulSofttMatmulinv and WastefulMatmulSofttMatmulinv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
