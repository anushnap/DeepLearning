{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6c0c55",
   "metadata": {},
   "source": [
    "Anushna Prakash  \n",
    "DATA 598 - Deep Learning  \n",
    "February 4th, 2022  \n",
    "# <center> Homework 4 </center>  \n",
    "## 1. Denoising AutoEncoders and Step Decay Learning Rates  \n",
    "\n",
    "- Use the `MNIST` dataset. Perform the same preprocessing as in this week’s lab.  \n",
    "- Use the same convolutional autoencoder as in this week’s lab, with a lower latent dimension of 40.  \n",
    "- As the corruption function $C( \\cdot)$, we zero out a randomly chosen $14 \\times 14$ patch in the original image. The code for this is provided in the lab again.  \n",
    "- Train the model for 40 epochs starting with $\\gamma_0 = 2.5 \\times 10^{−4}$ and take $t_{0} = 10$ (i.e., halve the learning rate every 10 epochs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f1b2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db818993",
   "metadata": {},
   "source": [
    "### Step 1: Import the data and preprocess.  \n",
    "Normalize the data over all the pixels, rather than a pixel-wise one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d744c1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = MNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = MNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10)).long()\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "f, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, idx in enumerate(np.random.choice(X_train.shape[0], 5)):\n",
    "    ax[i].imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)\n",
    "    ax[i].set_title(f'Label = {y_train[idx]}', fontsize=20)\n",
    "    \n",
    "# Normalize the data\n",
    "X_train = X_train.float()  # convert to float32\n",
    "# NOTE: we are returning a single mean/std over all the pixels, rather than a pixel-wise one\n",
    "mean, std = X_train.mean(), X_train.std()  \n",
    "X_train = (X_train - mean) / (std + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = (X_test - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843148cc",
   "metadata": {},
   "source": [
    "### Step 2: Create the AutoEncoder.  \n",
    "- `EncoderModule`  uses `stride=2`  \n",
    "- `DecoderModule` uses `stride=2` and `output_padding=1`  \n",
    "- `AutoEncoder` combines `EncoderModule` and `DecoderModule`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b78c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes batch images into lower dimensional space\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        # (B, 1, 28, 28) -> (B, 4, 12, 12)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=5, stride=2, padding=0) \n",
    "        # (B, 4, 12, 12) -> (B, 8, 5, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=0)\n",
    "        # Flatten (B, 8, 5, 5) -> (B, 8*5*5): do this in `forward()`\n",
    "        # (B, 8*5*5) -> (B, lower_dimension); 8*5*5 = 200\n",
    "        self.linear = torch.nn.Linear(200, lower_dimension)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        out = relu(self.conv1(images))  # conv1 + relu\n",
    "        out = relu(self.conv2(out))  # conv2 + relu\n",
    "        out = out.view(out.shape[0], -1)  # flatten\n",
    "        out = self.linear(out)  # Linear\n",
    "        return out\n",
    "    \n",
    "class DecoderModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes batch images back into original dimensional space\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        # (B, lower_dimension) -> (B, linear)\n",
    "        self.linear_t = torch.nn.Linear(lower_dimension, 200)\n",
    "        # Unflatten (B, 8*5*5) -> (B, 8, 5, 5); do this in `forward()`\n",
    "        # Exercise: plug in the output_padding values you determined above\n",
    "        # (B, 8, 5, 5) -> (B, 4, 12, 12)\n",
    "        self.conv2_t = torch.nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "        # (B, 4, 12, 12) -> (B, 1, 28, 28)\n",
    "        self.conv1_t = torch.nn.ConvTranspose2d(4, 1, kernel_size=5, stride=2, padding=0, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply in reverse order\n",
    "        out = relu(self.linear_t(x))  # linear_t + relu\n",
    "        out = out.view(out.shape[0], 8, 5, 5)  # Unflatten\n",
    "        out = relu(self.conv2_t(out))  # conv2_t + relu\n",
    "        out = self.conv1_t(out)  # conv1_t (note: no relu at the end)\n",
    "        return out\n",
    "\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Takes images in shape (B, 1, 28, 28)\n",
    "    Encodes down to (B, 8*5*5) for channels=8 of length lower_dimension\n",
    "    Decodes up from (B, 8*5*5) back to (B, 1, 28, 28)\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderModule(lower_dimension)\n",
    "        self.decoder = DecoderModule(lower_dimension)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        encoded_images = self.encoder(images)\n",
    "        decoded_images = self.decoder(encoded_images)\n",
    "        return decoded_images\n",
    "        \n",
    "    def encode_images(self, images):\n",
    "        \"\"\"\n",
    "        Encode images\n",
    "        \"\"\"\n",
    "        return self.encoder(images)\n",
    "    \n",
    "    def decode_representations(self, representations):\n",
    "        \"\"\"\n",
    "        Decode lower-dimensional representation\n",
    "        \"\"\"\n",
    "        return self.decoder(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72432d53",
   "metadata": {},
   "source": [
    "### Step 3: Corrupt images function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03c17e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_image_batch(images):\n",
    "    \"\"\"\n",
    "    Takes a batch of images and randomly zeros 14x14 square of pixels\n",
    "    \"\"\"\n",
    "    # images: (B, 1, 28, 28)\n",
    "    patch_size = 14  # zero out a 14x14 patch\n",
    "    batch_size = images.shape[0]\n",
    "    height, width = images.shape[-2:]  # height and width of each image\n",
    "    starting_h = np.random.choice(height - patch_size, size=batch_size, replace=True)\n",
    "    starting_w = np.random.choice(width - patch_size, size=batch_size, replace=True)\n",
    "\n",
    "    images_corrupted = images.clone()  # corrupt a copy so we do not lose the originals\n",
    "    for b in range(batch_size):\n",
    "        h = starting_h[b]\n",
    "        w = starting_w[b]\n",
    "        images_corrupted[b, 0, h:h+patch_size, b:b+patch_size] = 0  # set to 0\n",
    "    return images_corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f68cc8",
   "metadata": {},
   "source": [
    "### Step 4: Edit functions  \n",
    "$$\\min_{w,v} \\mathbb{E}_{x} = \\| x - g_{v} \\circ h_{w}(C(x)) \\| ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3502cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(true_images, reconstructed_images):\n",
    "    \"\"\"\n",
    "    Takes the true images and the reconstructed corrupted images\n",
    "    Returns the squared loss\n",
    "    \"\"\"\n",
    "    residual = (true_images - reconstructed_images).view(-1)  # flatten into a vector\n",
    "    # return the average over examples\n",
    "    return 0.5 * torch.norm(residual) ** 2 / (true_images.shape[0])\n",
    "\n",
    "def compute_objective(model, true_images, corrupted_images):\n",
    "    \"\"\"\n",
    "    Takes original images and corrupted images.\n",
    "    Returns the objective.\n",
    "    \"\"\"\n",
    "    reconstructed_images = model(corrupted_images)\n",
    "    return loss_function(true_images, reconstructed_images)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(\n",
    "    model, train_true_images, train_corrupted_images, # training variables\n",
    "    test_true_images, test_corrupted_images, # test variables\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Compute and return train and test loss\n",
    "    \"\"\"\n",
    "    train_loss = compute_objective(model, train_true_images, train_corrupted_images)\n",
    "    test_loss = compute_objective(model, test_true_images, test_corrupted_images)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Train Loss = {:.3f}, Test Loss = {:.3f}, '.format(\n",
    "                train_loss.item(), test_loss.item(),\n",
    "    ))\n",
    "    \n",
    "    return (train_loss, test_loss)\n",
    "\n",
    "def minibatch_sgd_one_pass(model, true_images, corrupted_images, learning_rate, batch_size, verbose=False):\n",
    "    num_examples = corrupted_images.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(num_examples, size=(batch_size,))\n",
    "        # compute the objective. \n",
    "        objective = compute_objective(model, true_images[idxs], corrupted_images[idxs]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(\"{:.3f}\".format(average_loss))\n",
    "         \n",
    "        # Perform the SGD update\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        with torch.no_grad():\n",
    "            for w, g in zip(model.parameters(), gradients):\n",
    "                w -= learning_rate * g\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3379104",
   "metadata": {},
   "source": [
    "### Step 5:  Train the model  \n",
    "- 40 epochs starting with $\\gamma_0 = 2.5 \\times 10^{−4}$ and take $t_{0} = 10$ (i.e., halve the learning rate every 10 epochs).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6a4e170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel = 1 dimension to data\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "# Create corrupted data\n",
    "C_X_train = corrupt_image_batch(X_train)\n",
    "C_X_test = corrupt_image_batch(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss = 401.571, Test Loss = 411.135, \n",
      "gamma = 0.00025\n",
      "Train Loss = 101.112, Test Loss = 100.862, \n",
      "Train Loss = 67.436, Test Loss = 67.231, \n",
      "Train Loss = 57.851, Test Loss = 57.434, \n",
      "Train Loss = 55.334, Test Loss = 55.137, \n",
      "Train Loss = 49.588, Test Loss = 49.338, \n",
      "Train Loss = 48.332, Test Loss = 48.109, \n",
      "Train Loss = 51.540, Test Loss = 51.625, \n",
      "Train Loss = 47.901, Test Loss = 47.673, \n",
      "Train Loss = 46.287, Test Loss = 46.219, \n",
      "Train Loss = 44.288, Test Loss = 44.219, \n",
      "gamma = 0.000125\n",
      "Train Loss = 41.163, Test Loss = 41.131, \n",
      "Train Loss = 41.059, Test Loss = 41.101, \n",
      "Train Loss = 41.011, Test Loss = 41.092, \n",
      "Train Loss = 40.540, Test Loss = 40.557, \n",
      "Train Loss = 41.041, Test Loss = 41.072, \n",
      "Train Loss = 40.017, Test Loss = 39.995, \n",
      "Train Loss = 40.833, Test Loss = 40.883, \n",
      "Train Loss = 39.367, Test Loss = 39.466, \n",
      "Train Loss = 39.621, Test Loss = 39.766, \n",
      "Train Loss = 39.375, Test Loss = 39.476, \n",
      "gamma = 6.25e-05\n",
      "Train Loss = 38.323, Test Loss = 38.447, \n",
      "Train Loss = 38.323, Test Loss = 38.477, \n",
      "Train Loss = 38.194, Test Loss = 38.288, \n",
      "Train Loss = 38.139, Test Loss = 38.286, \n",
      "Train Loss = 38.643, Test Loss = 38.851, \n",
      "Train Loss = 38.267, Test Loss = 38.439, \n",
      "Train Loss = 38.094, Test Loss = 38.281, \n",
      "Train Loss = 37.900, Test Loss = 38.098, \n",
      "Train Loss = 37.845, Test Loss = 38.020, \n",
      "Train Loss = 37.660, Test Loss = 37.854, \n",
      "gamma = 3.125e-05\n",
      "Train Loss = 37.296, Test Loss = 37.525, \n",
      "Train Loss = 37.280, Test Loss = 37.443, \n",
      "Train Loss = 37.276, Test Loss = 37.488, \n",
      "Train Loss = 37.166, Test Loss = 37.344, \n",
      "Train Loss = 37.236, Test Loss = 37.439, \n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "ae_model = AutoEncoder(lower_dimension=40)\n",
    "EPOCHS = 40\n",
    "T_0 = 10\n",
    "gamma = 2.5 * 10e-5 * 2 # This will be halved in the first iteration\n",
    "\n",
    "logs = []\n",
    "logs.append(compute_logs(ae_model, X_train, C_X_train, X_test, C_X_test, verbose=True))\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # Update learning rate parameter every T_0 epochs\n",
    "    if i % T_0 == 0:\n",
    "        gamma = gamma / 2\n",
    "        print(f'gamma = {gamma}')\n",
    "    \n",
    "    ae_model = minibatch_sgd_one_pass(ae_model, X_train, C_X_train, gamma, batch_size=1)\n",
    "    logs.append(compute_logs(ae_model, X_train, C_X_train, X_test, C_X_test, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a18b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16, 8))\n",
    "\n",
    "plt.plot(np.asarray(logs)[:, 0], label='Train')\n",
    "plt.plot(np.asarray(logs)[:, 1], label='Test')\n",
    "plt.legend(fontsize=12)\n",
    "plt.title('Loss over epochs', fontsize=12)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xlabel('Iterations', fontsize=12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
