{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1536cd3d",
   "metadata": {},
   "source": [
    "Anushna Prakash  \n",
    "DATA 598 - Deep Learning  \n",
    "February 4th, 2022  \n",
    "# <center> Homework 4 </center>  \n",
    "## 1. Denoising AutoEncoders and Step Decay Learning Rates  \n",
    "\n",
    "- Use the `MNIST` dataset. Perform the same preprocessing as in this week’s lab.  \n",
    "- Use the same convolutional autoencoder as in this week’s lab, with a lower latent dimension of 40.  \n",
    "- As the corruption function $C( \\cdot)$, we zero out a randomly chosen $14 \\times 14$ patch in the original image. The code for this is provided in the lab again.  \n",
    "- Train the model for 40 epochs starting with $\\gamma_0 = 2.5 \\times 10^{−4}$ and take $t_{0} = 10$ (i.e., halve the learning rate every 10 epochs).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e5dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d60ff1",
   "metadata": {},
   "source": [
    "### Step 1: Import the data and preprocess.  \n",
    "Normalize the data over all the pixels, rather than a pixel-wise one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a72aa774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = MNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = MNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10)).long()\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "f, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, idx in enumerate(np.random.choice(X_train.shape[0], 5)):\n",
    "    ax[i].imshow(X_train[idx], cmap='gray', vmin=0, vmax=255)\n",
    "    ax[i].set_title(f'Label = {y_train[idx]}', fontsize=20)\n",
    "    \n",
    "# Normalize the data\n",
    "X_train = X_train.float()  # convert to float32\n",
    "# NOTE: we are returning a single mean/std over all the pixels, rather than a pixel-wise one\n",
    "mean, std = X_train.mean(), X_train.std()  \n",
    "X_train = (X_train - mean) / (std + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = (X_test - mean) / (std + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9b5830",
   "metadata": {},
   "source": [
    "### Step 2: Create the AutoEncoder.  \n",
    "- `EncoderModule`  uses `stride=2`  \n",
    "- `DecoderModule` uses `stride=2` and `output_padding=1`  \n",
    "- `AutoEncoder` combines `EncoderModule` and `DecoderModule`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92361176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes batch images into lower dimensional space\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        # (B, 1, 28, 28) -> (B, 4, 12, 12)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, kernel_size=5, stride=2, padding=0) \n",
    "        # (B, 4, 12, 12) -> (B, 8, 5, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, kernel_size=3, stride=2, padding=0)\n",
    "        # Flatten (B, 8, 5, 5) -> (B, 8*5*5): do this in `forward()`\n",
    "        # (B, 8*5*5) -> (B, lower_dimension); 8*5*5 = 200\n",
    "        self.linear = torch.nn.Linear(200, lower_dimension)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        out = relu(self.conv1(images))  # conv1 + relu\n",
    "        out = relu(self.conv2(out))  # conv2 + relu\n",
    "        out = out.view(out.shape[0], -1)  # flatten\n",
    "        out = self.linear(out)  # Linear\n",
    "        return out\n",
    "    \n",
    "class DecoderModule(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes batch images back into original dimensional space\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        # (B, lower_dimension) -> (B, linear)\n",
    "        self.linear_t = torch.nn.Linear(lower_dimension, 200)\n",
    "        # Unflatten (B, 8*5*5) -> (B, 8, 5, 5); do this in `forward()`\n",
    "        # Exercise: plug in the output_padding values you determined above\n",
    "        # (B, 8, 5, 5) -> (B, 4, 12, 12)\n",
    "        self.conv2_t = torch.nn.ConvTranspose2d(8, 4, kernel_size=3, stride=2, padding=0, output_padding=1)\n",
    "        # (B, 4, 12, 12) -> (B, 1, 28, 28)\n",
    "        self.conv1_t = torch.nn.ConvTranspose2d(4, 1, kernel_size=5, stride=2, padding=0, output_padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply in reverse order\n",
    "        out = relu(self.linear_t(x))  # linear_t + relu\n",
    "        out = out.view(out.shape[0], 8, 5, 5)  # Unflatten\n",
    "        out = relu(self.conv2_t(out))  # conv2_t + relu\n",
    "        out = self.conv1_t(out)  # conv1_t (note: no relu at the end)\n",
    "        return out\n",
    "\n",
    "class AutoEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Takes images in shape (B, 1, 28, 28)\n",
    "    Encodes down to (B, 8*5*5) for channels=8 of length lower_dimension\n",
    "    Decodes up from (B, 8*5*5) back to (B, 1, 28, 28)\n",
    "    \"\"\"\n",
    "    def __init__(self, lower_dimension):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderModule(lower_dimension)\n",
    "        self.decoder = DecoderModule(lower_dimension)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        encoded_images = self.encoder(images)\n",
    "        decoded_images = self.decoder(encoded_images)\n",
    "        return decoded_images\n",
    "        \n",
    "    def encode_images(self, images):\n",
    "        \"\"\"\n",
    "        Encode images\n",
    "        \"\"\"\n",
    "        return self.encoder(images)\n",
    "    \n",
    "    def decode_representations(self, representations):\n",
    "        \"\"\"\n",
    "        Decode lower-dimensional representation\n",
    "        \"\"\"\n",
    "        return self.decoder(representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f09373",
   "metadata": {},
   "source": [
    "### Step 3: Corrupt images function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b620c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_image_batch(images):\n",
    "    \"\"\"\n",
    "    Takes a batch of images and randomly zeros 14x14 square of pixels\n",
    "    \"\"\"\n",
    "    # images: (B, 1, 28, 28)\n",
    "    patch_size = 14  # zero out a 14x14 patch\n",
    "    batch_size = images.shape[0]\n",
    "    height, width = images.shape[-2:]  # height and width of each image\n",
    "    starting_h = np.random.choice(height - patch_size, size=batch_size, replace=True)\n",
    "    starting_w = np.random.choice(width - patch_size, size=batch_size, replace=True)\n",
    "\n",
    "    images_corrupted = images.clone()  # corrupt a copy so we do not lose the originals\n",
    "    for b in range(batch_size):\n",
    "        h = starting_h[b]\n",
    "        w = starting_w[b]\n",
    "        images_corrupted[b, 0, h:h+patch_size, b:b+patch_size] = 0  # set to 0\n",
    "    return images_corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ef3b3",
   "metadata": {},
   "source": [
    "### Step 4: Edit functions  \n",
    "$$\\min_{w,v} \\mathbb{E}_{x} = \\| x - g_{v} \\circ h_{w}(C(x)) \\| ^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "615deb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(true_images, reconstructed_images):\n",
    "    \"\"\"\n",
    "    Takes the true images and the reconstructed corrupted images\n",
    "    Returns the squared loss\n",
    "    \"\"\"\n",
    "    residual = (true_images - reconstructed_images).view(-1)  # flatten into a vector\n",
    "    # return the average over examples\n",
    "    return 0.5 * torch.norm(residual) ** 2 / (true_images.shape[0])\n",
    "\n",
    "def compute_objective(model, true_images, corrupted_images):\n",
    "    \"\"\"\n",
    "    Takes original images and corrupted images.\n",
    "    Returns the objective.\n",
    "    \"\"\"\n",
    "    reconstructed_images = model(corrupted_images)\n",
    "    return loss_function(true_images, reconstructed_images)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_logs(\n",
    "    model, train_true_images, train_corrupted_images, # training variables\n",
    "    test_true_images, test_corrupted_images, # test variables\n",
    "    verbose=False):\n",
    "    \"\"\"\n",
    "    Compute and return train and test loss\n",
    "    \"\"\"\n",
    "    train_loss = compute_objective(model, train_true_images, train_corrupted_images)\n",
    "    test_loss = compute_objective(model, test_true_images, test_corrupted_images)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Train Loss = {:.3f}, Test Loss = {:.3f}, '.format(\n",
    "                train_loss.item(), test_loss.item(),\n",
    "    ))\n",
    "    \n",
    "    return (train_loss, test_loss)\n",
    "\n",
    "def minibatch_sgd_one_pass(model, true_images, corrupted_images, learning_rate, batch_size, verbose=False):\n",
    "    num_examples = corrupted_images.shape[0]\n",
    "    average_loss = 0.0\n",
    "    num_updates = int(round(num_examples / batch_size))\n",
    "    for i in range(num_updates):\n",
    "        idxs = np.random.choice(num_examples, size=(batch_size,)) \n",
    "        # compute the objective. \n",
    "        objective = compute_objective(model, true_images[idxs], corrupted_images[idxs]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(\"{:.3f}\".format(average_loss))\n",
    "         \n",
    "        # Perform the SGD update\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        with torch.no_grad():\n",
    "            for w, g in zip(model.parameters(), gradients):\n",
    "                w -= learning_rate * g\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7773da6",
   "metadata": {},
   "source": [
    "### Step 5:  Train the model  \n",
    "- 40 epochs starting with 40 epochs starting with $\\gamma_0 = 2.5 \\times 10^{−4}$ and take $t_{0} = 10$ (i.e., halve the learning rate every 10 epochs).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6778cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add channel = 1 dimension to data\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)\n",
    "# Create corrupted data\n",
    "C_X_train = corrupt_image_batch(X_train)\n",
    "C_X_test = corrupt_image_batch(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bc94ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0.0025\n",
      "Train Loss = 392.438, Test Loss = 399.555, \n",
      "Train Loss = 391.401, Test Loss = 391.805, \n",
      "Train Loss = 395.187, Test Loss = 402.539, \n",
      "Train Loss = 407.890, Test Loss = 413.653, \n",
      "Train Loss = 391.516, Test Loss = 405.837, \n",
      "Train Loss = 392.425, Test Loss = 400.932, \n",
      "Train Loss = 400.350, Test Loss = 413.008, \n",
      "Train Loss = 415.979, Test Loss = 427.410, \n",
      "Train Loss = 400.459, Test Loss = 413.219, \n",
      "Train Loss = 412.117, Test Loss = 412.132, \n",
      "gamma = 0.00125\n",
      "Train Loss = 390.246, Test Loss = 390.250, \n",
      "Train Loss = 392.583, Test Loss = 394.328, \n",
      "Train Loss = 392.858, Test Loss = 393.933, \n",
      "Train Loss = 393.026, Test Loss = 407.758, \n",
      "Train Loss = 390.779, Test Loss = 391.297, \n",
      "Train Loss = 393.317, Test Loss = 395.689, \n",
      "Train Loss = 393.646, Test Loss = 408.238, \n",
      "Train Loss = 392.087, Test Loss = 392.495, \n",
      "Train Loss = 390.229, Test Loss = 390.241, \n",
      "Train Loss = 391.884, Test Loss = 392.288, \n",
      "gamma = 0.000625\n",
      "Train Loss = 392.593, Test Loss = 393.439, \n",
      "Train Loss = 392.359, Test Loss = 406.940, \n",
      "Train Loss = 392.988, Test Loss = 394.020, \n",
      "Train Loss = 392.466, Test Loss = 393.340, \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_308/1524367858.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mae_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch_sgd_one_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_X_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_308/1619483070.py\u001b[0m in \u001b[0;36mcompute_logs\u001b[0;34m(model, train_true_images, train_corrupted_images, test_true_images, test_corrupted_images, verbose)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \"\"\"\n\u001b[1;32m     26\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_true_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_corrupted_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_true_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corrupted_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_308/1619483070.py\u001b[0m in \u001b[0;36mcompute_objective\u001b[0;34m(model, true_images, corrupted_images)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \"\"\"\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mreconstructed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstructed_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_308/2470749355.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mencoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mdecoded_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_308/2470749355.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# conv1 + relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# conv2 + relu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# flatten\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data598/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "ae_model = AutoEncoder(lower_dimension=40)\n",
    "EPOCHS = 40\n",
    "T_0 = 10\n",
    "gamma = 2.5 * 10e-4 * 2 # This will be halved in the first iteration\n",
    "\n",
    "logs = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    # Update learning rate parameter every T_0 epochs\n",
    "    if i % T_0 == 0:\n",
    "        gamma = gamma / 2\n",
    "        print(f'gamma = {gamma}')\n",
    "    \n",
    "    ae_model = minibatch_sgd_one_pass(ae_model, X_train, C_X_train, gamma, batch_size=30)\n",
    "    logs.append(compute_logs(ae_model, X_train, C_X_train, X_test, C_X_test, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d042b3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.2459, -0.2468,  0.0518,  0.1091, -0.0537],\n",
      "          [ 0.0963, -0.2152, -0.0923, -0.1972, -0.1904],\n",
      "          [ 0.0290, -0.0073, -0.2862,  0.0224,  0.0071],\n",
      "          [ 0.0140, -0.0609,  0.0318, -0.2038,  0.0062],\n",
      "          [ 0.0230, -0.2118,  0.1075,  0.0235,  0.0088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0902,  0.0439,  0.1023,  0.1637, -0.1305],\n",
      "          [ 0.0940,  0.0282,  0.0247, -0.1364,  0.0315],\n",
      "          [-0.0281, -0.0233, -0.0955,  0.1759, -0.1101],\n",
      "          [-0.2194,  0.0787,  0.0103, -0.0232, -0.1381],\n",
      "          [ 0.0318,  0.1082, -0.1303, -0.0102,  0.1548]]],\n",
      "\n",
      "\n",
      "        [[[-0.2239,  0.0642, -0.1326, -0.0068, -0.2131],\n",
      "          [-0.1276, -0.1838, -0.1168,  0.0452, -0.0752],\n",
      "          [ 0.0228, -0.1218,  0.0203, -0.1568, -0.1282],\n",
      "          [ 0.0582, -0.0148, -0.2560, -0.2911, -0.2251],\n",
      "          [ 0.0785, -0.2517, -0.0291, -0.1072,  0.0570]]],\n",
      "\n",
      "\n",
      "        [[[-0.1256, -0.0495,  0.0013, -0.0275, -0.1496],\n",
      "          [ 0.0552, -0.1722,  0.1169,  0.1269, -0.2001],\n",
      "          [ 0.0736, -0.1270,  0.1358, -0.0017, -0.1683],\n",
      "          [-0.1626, -0.1706,  0.1099, -0.0615, -0.2043],\n",
      "          [ 0.0826, -0.2329, -0.1885,  0.0160,  0.1740]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0867, -0.1070, -0.1587,  0.0664], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-6.2964e-02,  1.0991e-02, -6.7638e-02],\n",
      "          [-1.5734e-01,  5.3107e-02,  3.0899e-02],\n",
      "          [-2.4629e-01,  1.5868e-02, -5.4448e-02]],\n",
      "\n",
      "         [[ 1.3850e-01, -3.7582e-02, -7.4358e-02],\n",
      "          [-8.0684e-02, -1.2726e-01, -2.1344e-02],\n",
      "          [ 8.3545e-02,  1.1401e-01, -6.0711e-02]],\n",
      "\n",
      "         [[-2.5290e-01,  2.4609e-03, -2.9823e-03],\n",
      "          [-2.2310e-01, -1.1507e-03, -4.0180e-02],\n",
      "          [-1.7393e-01, -2.4804e-01, -1.9466e-01]],\n",
      "\n",
      "         [[ 5.9474e-02, -1.6826e-01,  5.7264e-02],\n",
      "          [ 9.3896e-02, -7.6772e-02, -1.5491e-01],\n",
      "          [-7.9828e-02,  4.4356e-02,  1.6398e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6875e-02,  4.2721e-02, -7.3531e-02],\n",
      "          [ 1.1602e-01, -4.8304e-02, -1.0382e-01],\n",
      "          [-1.5298e-01, -5.6881e-02, -6.3683e-03]],\n",
      "\n",
      "         [[ 9.2672e-03, -9.1934e-02,  1.8284e-03],\n",
      "          [ 1.2520e-01,  1.6032e-01, -7.2968e-02],\n",
      "          [ 5.6774e-02,  8.1799e-02, -1.2647e-01]],\n",
      "\n",
      "         [[-7.2278e-02,  8.2042e-03, -2.9193e-02],\n",
      "          [-3.2662e-02, -1.0452e-01,  4.8323e-02],\n",
      "          [ 1.3135e-02,  3.6162e-02,  1.7831e-02]],\n",
      "\n",
      "         [[-1.4145e-02,  1.2284e-01, -5.1085e-02],\n",
      "          [-4.9015e-02, -1.4707e-01, -7.9003e-02],\n",
      "          [-1.3004e-01, -1.1369e-01,  1.0626e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5258e-02, -2.0913e-02, -1.1034e-01],\n",
      "          [ 5.5118e-02,  1.0767e-01,  9.3296e-06],\n",
      "          [-1.5662e-01,  3.1683e-02,  9.1583e-02]],\n",
      "\n",
      "         [[ 3.6212e-02, -1.2900e-01,  1.4495e-01],\n",
      "          [-2.5211e-02, -1.6513e-01, -5.5083e-02],\n",
      "          [-1.2953e-01,  1.2942e-01,  5.9014e-02]],\n",
      "\n",
      "         [[ 8.6828e-03,  9.4750e-02, -1.0714e-01],\n",
      "          [-1.4403e-01, -1.4968e-01, -4.2663e-02],\n",
      "          [ 2.5216e-02, -1.4399e-01,  5.4963e-02]],\n",
      "\n",
      "         [[ 5.8952e-02, -7.9007e-02, -1.1738e-01],\n",
      "          [ 2.3588e-02, -1.2312e-01, -4.6028e-02],\n",
      "          [-4.1773e-03,  1.0403e-01, -1.5265e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.4966e-01, -1.8405e-01, -7.7991e-02],\n",
      "          [ 4.2028e-03,  1.5676e-02, -1.1864e-01],\n",
      "          [-6.0892e-02, -1.2764e-02, -1.1812e-01]],\n",
      "\n",
      "         [[ 3.2450e-03,  9.4538e-02, -1.1820e-01],\n",
      "          [ 1.3666e-01,  8.5493e-02, -5.1741e-02],\n",
      "          [-1.1660e-01,  5.5109e-02, -7.5672e-02]],\n",
      "\n",
      "         [[-1.0954e-01, -5.2806e-03, -1.0841e-01],\n",
      "          [-1.7511e-01, -2.2100e-01, -2.6909e-02],\n",
      "          [-4.2321e-02, -5.7954e-02, -5.0746e-02]],\n",
      "\n",
      "         [[-5.7961e-02, -9.4503e-02, -3.9555e-02],\n",
      "          [-1.7447e-01,  1.0666e-01,  6.7509e-02],\n",
      "          [ 6.8685e-02,  1.4675e-01,  7.9381e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.8628e-02,  9.9154e-02, -8.8099e-02],\n",
      "          [ 1.3296e-01,  3.5054e-02, -1.1604e-01],\n",
      "          [ 1.1352e-01, -6.9521e-02,  1.7926e-02]],\n",
      "\n",
      "         [[ 1.8823e-02,  1.0697e-02,  1.0076e-01],\n",
      "          [ 3.8417e-02, -5.1619e-02, -2.0597e-02],\n",
      "          [-1.4928e-01,  1.0125e-01, -8.6864e-02]],\n",
      "\n",
      "         [[-1.2480e-01,  9.4418e-02,  5.6070e-02],\n",
      "          [-2.0380e-01, -3.7460e-02, -2.0469e-01],\n",
      "          [-1.0574e-01,  1.6839e-02, -4.4462e-02]],\n",
      "\n",
      "         [[ 5.0579e-02, -4.9902e-02, -1.8744e-02],\n",
      "          [ 6.8649e-02,  8.8112e-02, -1.2229e-01],\n",
      "          [ 6.1614e-02,  3.7289e-02,  1.0863e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4948e-01, -1.1977e-02, -8.6496e-02],\n",
      "          [-1.1740e-02, -1.6988e-01, -5.6985e-02],\n",
      "          [-1.6775e-01,  1.3718e-01,  2.5039e-02]],\n",
      "\n",
      "         [[-9.1413e-02,  1.7714e-02,  1.4023e-01],\n",
      "          [-1.2212e-01,  5.6705e-02, -7.3228e-02],\n",
      "          [-3.5694e-02,  1.2937e-01, -8.1235e-02]],\n",
      "\n",
      "         [[-1.1771e-01, -1.2075e-01, -6.7876e-02],\n",
      "          [-1.3090e-01, -1.3310e-01, -4.9889e-02],\n",
      "          [ 4.0430e-02,  1.2635e-01,  7.4713e-02]],\n",
      "\n",
      "         [[ 8.9223e-02, -8.1343e-02, -1.0986e-01],\n",
      "          [ 4.5759e-02,  1.3233e-01, -7.7841e-02],\n",
      "          [ 5.4161e-02, -5.9426e-02,  1.5659e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5809e-02,  4.2507e-02, -2.5231e-02],\n",
      "          [-2.1102e-02, -2.1878e-02, -7.4694e-02],\n",
      "          [ 7.1584e-02,  7.8605e-03, -3.3114e-02]],\n",
      "\n",
      "         [[ 1.5211e-01, -8.3155e-02,  1.1532e-01],\n",
      "          [ 9.6213e-02,  1.3791e-01, -1.1562e-01],\n",
      "          [-7.4165e-02, -2.2517e-02,  4.3350e-02]],\n",
      "\n",
      "         [[-4.4822e-02, -1.5422e-01,  6.6010e-02],\n",
      "          [ 7.8103e-02, -1.7640e-01,  1.1453e-01],\n",
      "          [-1.0295e-01, -1.4760e-01, -1.1584e-01]],\n",
      "\n",
      "         [[ 1.0427e-02,  1.0236e-01,  3.8738e-02],\n",
      "          [ 1.4141e-01, -4.1817e-02,  3.3574e-02],\n",
      "          [-6.7310e-02, -5.6370e-02,  1.4244e-01]]],\n",
      "\n",
      "\n",
      "        [[[-1.3154e-01,  1.2220e-02,  8.8610e-02],\n",
      "          [-1.9413e-01, -9.4409e-02, -1.5963e-01],\n",
      "          [ 8.3241e-03, -1.1559e-01, -7.9821e-02]],\n",
      "\n",
      "         [[-1.0279e-01,  2.4041e-02, -1.5820e-01],\n",
      "          [-7.9129e-03, -5.9156e-02, -1.4442e-01],\n",
      "          [-2.1835e-02, -6.8960e-02,  1.0002e-01]],\n",
      "\n",
      "         [[-1.2297e-01, -7.5116e-02, -8.6481e-02],\n",
      "          [ 5.5991e-02,  8.2569e-02, -1.1967e-02],\n",
      "          [-1.6039e-01, -1.3674e-02,  8.8796e-02]],\n",
      "\n",
      "         [[ 2.0987e-04,  4.8454e-02,  2.2667e-02],\n",
      "          [-3.5944e-02,  1.0502e-01, -1.8791e-02],\n",
      "          [-1.5971e-01,  2.7489e-02,  1.5926e-01]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0149,  0.1486,  0.0486, -0.0589,  0.1876, -0.0923,  0.1046,  0.0616],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0215, -0.0126,  0.0124,  ..., -0.0461,  0.0459,  0.0268],\n",
      "        [-0.0203,  0.0582, -0.0448,  ..., -0.0193, -0.0621,  0.0449],\n",
      "        [-0.0102,  0.0163,  0.0318,  ...,  0.0405, -0.0020, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0039, -0.0301, -0.0573,  ..., -0.0056, -0.0449,  0.0275],\n",
      "        [-0.0515, -0.0207, -0.0479,  ...,  0.0208, -0.0482, -0.0508],\n",
      "        [ 0.0181,  0.0413,  0.0383,  ...,  0.0365, -0.0511,  0.0525]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0794,  0.0150, -0.0716, -0.0583, -0.0306, -0.0482, -0.0709, -0.1078,\n",
      "         0.0009, -0.0665,  0.0371, -0.0217, -0.0270,  0.0824,  0.0248,  0.0029,\n",
      "        -0.0431,  0.0237, -0.0401,  0.0057, -0.0382, -0.0657,  0.0006, -0.0674,\n",
      "         0.0224,  0.0425, -0.0112, -0.0578,  0.0177,  0.1029, -0.0559, -0.0409,\n",
      "        -0.0092,  0.0508, -0.0566, -0.0873, -0.0299,  0.0890, -0.0858, -0.0444],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0679, -0.0425, -0.1082,  ..., -0.0475,  0.1404, -0.1243],\n",
      "        [ 0.0918, -0.0229, -0.0721,  ...,  0.1168, -0.0580,  0.0978],\n",
      "        [ 0.1017,  0.1121,  0.0661,  ...,  0.0386, -0.0014, -0.0358],\n",
      "        ...,\n",
      "        [-0.0777, -0.0443,  0.0814,  ..., -0.1322, -0.0739, -0.0966],\n",
      "        [-0.0530,  0.1059,  0.0450,  ..., -0.0794,  0.0082, -0.1480],\n",
      "        [-0.0976,  0.0147, -0.0950,  ...,  0.1367,  0.1366,  0.0601]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0158, -0.0274, -0.0602,  0.0330,  0.0564,  0.1166,  0.1234, -0.1218,\n",
      "        -0.1137, -0.0295,  0.0596,  0.0290, -0.0355, -0.1303, -0.0111, -0.0517,\n",
      "         0.0882, -0.0913,  0.0874,  0.0688,  0.2006,  0.0491,  0.0744, -0.0390,\n",
      "         0.0431, -0.0639,  0.0209, -0.0062, -0.1168,  0.0092,  0.1265,  0.0888,\n",
      "         0.0358, -0.0341, -0.1207,  0.0020, -0.0716, -0.0687,  0.0770, -0.0105,\n",
      "         0.0717, -0.0679, -0.0195,  0.1415,  0.1242,  0.0586,  0.0576,  0.1276,\n",
      "         0.1713,  0.0302, -0.0569, -0.0512, -0.0072,  0.0109,  0.1521, -0.1567,\n",
      "         0.1119, -0.0112, -0.0571,  0.1497, -0.1475, -0.1404,  0.0291, -0.1443,\n",
      "         0.1315,  0.1164, -0.0159,  0.0496, -0.0549,  0.0781, -0.1153,  0.0404,\n",
      "        -0.0501, -0.0634, -0.1151,  0.0459,  0.0227, -0.0895,  0.0959, -0.1493,\n",
      "         0.1192, -0.0410,  0.0425, -0.0999, -0.1160, -0.1523,  0.0683,  0.1435,\n",
      "         0.1717,  0.1505, -0.0412, -0.0807,  0.1154,  0.1620, -0.0373,  0.0639,\n",
      "        -0.0998,  0.1633,  0.0258,  0.0559,  0.0263, -0.1457, -0.0759, -0.0714,\n",
      "         0.0086,  0.0407, -0.0993,  0.1613, -0.0666, -0.0252, -0.1415,  0.0166,\n",
      "         0.1047, -0.0613, -0.0825, -0.0020, -0.1070,  0.0054, -0.1378, -0.0860,\n",
      "        -0.1044,  0.1348,  0.0909,  0.1189,  0.0220, -0.1130, -0.1437, -0.0695,\n",
      "         0.0703,  0.0544,  0.0409,  0.0538, -0.1362,  0.0743,  0.0207, -0.1321,\n",
      "        -0.0976,  0.1278,  0.1353, -0.0110, -0.0360,  0.0107,  0.1556,  0.0460,\n",
      "         0.1377, -0.1188,  0.0408,  0.1247, -0.1023,  0.1130, -0.0220,  0.0152,\n",
      "        -0.0794,  0.0852, -0.0482, -0.1097,  0.0014,  0.1318,  0.0440, -0.0567,\n",
      "        -0.0080, -0.0020,  0.0600,  0.0355,  0.0432,  0.0434,  0.1136, -0.0838,\n",
      "         0.0941, -0.0629,  0.0958, -0.1082, -0.0568, -0.0276,  0.0175,  0.1170,\n",
      "         0.1264,  0.0944, -0.0826,  0.1593,  0.2286,  0.0392, -0.0854, -0.1145,\n",
      "        -0.0774,  0.1749, -0.0936, -0.1292,  0.0342,  0.0095,  0.0301, -0.1478,\n",
      "         0.0512, -0.1322, -0.1254,  0.0604,  0.0305,  0.0585, -0.0170, -0.0603],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[-0.0607, -0.1220, -0.0060],\n",
      "          [-0.1318,  0.0585, -0.0701],\n",
      "          [ 0.1228, -0.0276, -0.1381]],\n",
      "\n",
      "         [[-0.1585, -0.1162,  0.0418],\n",
      "          [-0.1560, -0.1006, -0.1079],\n",
      "          [-0.0804, -0.2049, -0.1910]],\n",
      "\n",
      "         [[ 0.0852, -0.0456, -0.1974],\n",
      "          [-0.1768, -0.1469, -0.1326],\n",
      "          [-0.1107, -0.0557, -0.1470]],\n",
      "\n",
      "         [[-0.1431,  0.2778, -0.1305],\n",
      "          [ 0.0670,  0.0729, -0.1573],\n",
      "          [-0.1194,  0.1777, -0.1373]]],\n",
      "\n",
      "\n",
      "        [[[-0.0340, -0.0308,  0.0925],\n",
      "          [-0.0573, -0.1313, -0.0270],\n",
      "          [ 0.1071,  0.0187,  0.0328]],\n",
      "\n",
      "         [[ 0.1272,  0.2012,  0.0032],\n",
      "          [ 0.0051,  0.0222, -0.0512],\n",
      "          [ 0.0622,  0.0670,  0.2562]],\n",
      "\n",
      "         [[-0.1038, -0.1195, -0.0123],\n",
      "          [-0.0221,  0.0372,  0.0434],\n",
      "          [-0.1917, -0.1373,  0.0379]],\n",
      "\n",
      "         [[-0.0581,  0.0914,  0.0489],\n",
      "          [-0.0551, -0.1277, -0.1712],\n",
      "          [ 0.0323, -0.0648, -0.1069]]],\n",
      "\n",
      "\n",
      "        [[[-0.0897, -0.0316,  0.1204],\n",
      "          [ 0.0345,  0.0607, -0.0214],\n",
      "          [-0.0645, -0.0285, -0.0321]],\n",
      "\n",
      "         [[ 0.0097, -0.1909, -0.1763],\n",
      "          [-0.0397,  0.0408,  0.1037],\n",
      "          [-0.1679, -0.1296, -0.0777]],\n",
      "\n",
      "         [[ 0.0686, -0.1386,  0.0756],\n",
      "          [ 0.0222, -0.0436,  0.0657],\n",
      "          [ 0.0670, -0.0776, -0.1295]],\n",
      "\n",
      "         [[-0.1279, -0.0413, -0.0573],\n",
      "          [-0.0994,  0.0961, -0.0546],\n",
      "          [ 0.0111,  0.0281,  0.0239]]],\n",
      "\n",
      "\n",
      "        [[[-0.1297, -0.1584, -0.1290],\n",
      "          [ 0.1241,  0.1116,  0.1360],\n",
      "          [ 0.0427,  0.0486, -0.0992]],\n",
      "\n",
      "         [[ 0.0983,  0.0117,  0.0944],\n",
      "          [-0.1060,  0.0578, -0.0167],\n",
      "          [ 0.0449,  0.1091, -0.0318]],\n",
      "\n",
      "         [[ 0.0214, -0.1765, -0.1194],\n",
      "          [ 0.1256,  0.1077, -0.1863],\n",
      "          [-0.2177,  0.0724, -0.1488]],\n",
      "\n",
      "         [[-0.1689, -0.1424,  0.0621],\n",
      "          [ 0.0097,  0.0238,  0.1095],\n",
      "          [ 0.0644, -0.0964, -0.0635]]],\n",
      "\n",
      "\n",
      "        [[[-0.1318,  0.1185,  0.0567],\n",
      "          [ 0.1354, -0.0018, -0.0487],\n",
      "          [-0.0738, -0.1012, -0.1093]],\n",
      "\n",
      "         [[ 0.1507, -0.1230,  0.0286],\n",
      "          [-0.1198, -0.0916, -0.1140],\n",
      "          [ 0.1712,  0.1341,  0.1991]],\n",
      "\n",
      "         [[-0.0703,  0.0361,  0.0061],\n",
      "          [ 0.1460, -0.0988, -0.0142],\n",
      "          [-0.0334, -0.0398, -0.1317]],\n",
      "\n",
      "         [[-0.0028, -0.0536, -0.0913],\n",
      "          [ 0.0529, -0.1814, -0.0652],\n",
      "          [-0.0019, -0.0495, -0.0605]]],\n",
      "\n",
      "\n",
      "        [[[-0.1287,  0.0423, -0.0280],\n",
      "          [-0.0532,  0.0032, -0.0508],\n",
      "          [-0.0535,  0.0087,  0.0013]],\n",
      "\n",
      "         [[-0.1608,  0.0066,  0.1040],\n",
      "          [ 0.0348,  0.1360,  0.0459],\n",
      "          [-0.0548, -0.0335, -0.0422]],\n",
      "\n",
      "         [[-0.1224,  0.1024, -0.0435],\n",
      "          [-0.0379, -0.0844, -0.0700],\n",
      "          [-0.1073, -0.1415, -0.0525]],\n",
      "\n",
      "         [[-0.0587, -0.1919, -0.0718],\n",
      "          [-0.0518, -0.0938, -0.0880],\n",
      "          [-0.1541, -0.0180, -0.1274]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.1157,  0.1046],\n",
      "          [ 0.0188, -0.0225,  0.0434],\n",
      "          [-0.1726,  0.0293, -0.0304]],\n",
      "\n",
      "         [[-0.0547, -0.0940, -0.1539],\n",
      "          [-0.0581, -0.0971,  0.0709],\n",
      "          [-0.1566,  0.0201, -0.1101]],\n",
      "\n",
      "         [[-0.1860,  0.0108,  0.0779],\n",
      "          [-0.1519, -0.1122,  0.1255],\n",
      "          [-0.0560, -0.0892, -0.1154]],\n",
      "\n",
      "         [[-0.0147,  0.0281,  0.0047],\n",
      "          [-0.1289, -0.0087, -0.0628],\n",
      "          [-0.0481, -0.1239,  0.0145]]],\n",
      "\n",
      "\n",
      "        [[[-0.1388,  0.0722,  0.0033],\n",
      "          [-0.1298, -0.0493,  0.0169],\n",
      "          [ 0.0552, -0.0923,  0.0100]],\n",
      "\n",
      "         [[-0.2712,  0.0031, -0.2322],\n",
      "          [-0.1399, -0.1744, -0.1375],\n",
      "          [-0.2392, -0.2257, -0.1449]],\n",
      "\n",
      "         [[-0.2153, -0.1778, -0.2368],\n",
      "          [-0.1321, -0.0668, -0.0667],\n",
      "          [-0.0403, -0.1659,  0.0333]],\n",
      "\n",
      "         [[-0.0170,  0.4340,  0.1620],\n",
      "          [ 0.1745,  0.3230, -0.0591],\n",
      "          [-0.1233,  0.1123,  0.0279]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1896, -0.8536, -0.6201, -0.3516], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[[[ 0.1415, -0.1161,  0.1011,  0.0243,  0.1032],\n",
      "          [-0.0819, -0.0342,  0.1094, -0.0062, -0.0736],\n",
      "          [ 0.1554, -0.0432,  0.0442,  0.0565, -0.0069],\n",
      "          [-0.0157, -0.1351, -0.0770, -0.0747, -0.1834],\n",
      "          [ 0.1153, -0.1483,  0.1318, -0.1415,  0.0359]]],\n",
      "\n",
      "\n",
      "        [[[-0.0058, -0.0240, -0.0941, -0.2273, -0.2781],\n",
      "          [-0.2512,  0.0043, -0.1026, -0.1440, -0.0748],\n",
      "          [-0.2715, -0.1066, -0.2483, -0.1153, -0.3088],\n",
      "          [-0.0320, -0.1502, -0.0930,  0.0142, -0.3975],\n",
      "          [-0.3115, -0.2474, -0.2360, -0.2147, -0.2566]]],\n",
      "\n",
      "\n",
      "        [[[-0.2768, -0.1728, -0.1627, -0.1927, -0.2514],\n",
      "          [-0.1230, -0.1680, -0.2329, -0.0451, -0.0045],\n",
      "          [-0.1461, -0.1613, -0.0866, -0.1088, -0.0884],\n",
      "          [-0.0191, -0.0048, -0.0737,  0.0649, -0.0649],\n",
      "          [-0.0300,  0.0188, -0.1728,  0.0044, -0.1616]]],\n",
      "\n",
      "\n",
      "        [[[-0.0462, -0.0919, -0.1414, -0.2446, -0.2675],\n",
      "          [-0.2118, -0.1692, -0.2185, -0.1832, -0.0592],\n",
      "          [-0.1959, -0.1771, -0.1692, -0.1009,  0.0180],\n",
      "          [-0.1168, -0.1389, -0.0393,  0.0299,  0.0831],\n",
      "          [-0.1090, -0.0779,  0.0167,  0.1768,  0.2874]]]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3541], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in ae_model.parameters():\n",
    "    print(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
