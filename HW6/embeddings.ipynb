{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Lecture 6: Embeddings and Model Selection </h1> </center>\n",
    "<center> Anushna Prakash </center>\n",
    "    <center> Data 598 (Winter 2022), University of Washington </center>\n",
    "\n",
    "## This is my submission for Assignment 6. The Bonus part has been filled in towards the bottom of the notebook.\n",
    "\n",
    "We will discuss two topics this lecture:\n",
    "- Embeddings for natural language\n",
    "- Model Selection with statistical tests\n",
    "\n",
    "\n",
    "The first part of this notebook has been adapted from the [D2L book]( http://d2l.ai/chapter_natural-language-processing-pretraining/similarity-analogy.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Embeddings for Natural Language\n",
    "\n",
    "The field of **natural language processing (NLP)** is concerned with the interaction between computers and natural (human) language. This involves \"understanding\" the contents of documents, including the contextual nuances of the language within them. \n",
    "\n",
    "**Embeddings**:\n",
    "The use of machine learning for NLP, both in the classical settings as well as the modern deep learning era, have relied on *embedding* words in vector spaces.\n",
    "Words are made of characters, which are combinatorial in nature with no \"neighborhood\" structure which one expects of vectors in, say, a Euclidean space. \n",
    "The magic of embeddings is that they are able to capture some \"neighborhood\" structure in words, e.g., the embedding of synonyms are closer together than of words which have nothing in common. \n",
    "\n",
    "![](https://miro.medium.com/max/2400/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
    "Image credits: https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
    "\n",
    "**Note**: Sometimes, we will work at the level of subword units, rather than words. Mathematically, the same treatment holds irrespective of how we *tokenize* the text. We will refer to these units as *tokens*.\n",
    "\n",
    "\n",
    "**Types of embeddings**:\n",
    "\n",
    "- Global embeddings: word2vec, GloVe\n",
    "- Contextual embeddings: ELMo, BERT, ...\n",
    "\n",
    "In this lab, we will play with the GloVe embeddings, which are global embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-03-01 20:56:34--  http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip\n",
      "Resolving d2l-data.s3-accelerate.amazonaws.com (d2l-data.s3-accelerate.amazonaws.com)... 52.84.162.136\n",
      "Connecting to d2l-data.s3-accelerate.amazonaws.com (d2l-data.s3-accelerate.amazonaws.com)|52.84.162.136|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69182829 (66M) [application/zip]\n",
      "Saving to: ‘glove.6B.50d.zip’\n",
      "\n",
      "glove.6B.50d.zip    100%[===================>]  65.98M  9.75MB/s    in 6.8s    \n",
      "\n",
      "2022-03-01 20:56:41 (9.64 MB/s) - ‘glove.6B.50d.zip’ saved [69182829/69182829]\n",
      "\n",
      "Archive:  glove.6B.50d.zip\n",
      "   creating: glove.6B.50d/\n",
      "  inflating: glove.6B.50d/vec.txt    \n"
     ]
    }
   ],
   "source": [
    "# Download the GloVe embedding ~66M compressed + 164M uncompressed\n",
    "import os\n",
    "if 'glove.6B.50d' not in os.listdir():\n",
    "    !wget http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip\n",
    "    !unzip glove.6B.50d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We index each word in our dictionary using integers. \n",
    "We store the following mappings:\n",
    "- word $\\to$ index\n",
    "- index $\\to$ word\n",
    "- index $\\to$ embedding of the corresponding word\n",
    "\n",
    "Words not in our dictionary are denoted using the `<unk>` token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVeWordEmbedding:\n",
    "    def __init__(self):\n",
    "        self.idx_to_token, self.idx_to_vec = self._load_embedding()\n",
    "        self.unknown_idx = 0\n",
    "        self.token_to_idx = {token: idx for idx, token in\n",
    "                             enumerate(self.idx_to_token)}\n",
    "\n",
    "    def _load_embedding(self):\n",
    "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
    "        with open('glove.6B.50d/vec.txt') as f:\n",
    "            for line in f:\n",
    "                elems = line.rstrip().split(' ')\n",
    "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
    "                # Skip header information, such as the top row in fastText\n",
    "                if len(elems) > 1:\n",
    "                    idx_to_token.append(token)\n",
    "                    idx_to_vec.append(elems)\n",
    "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
    "        return idx_to_token, np.asarray(idx_to_vec)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        # \"tokens\" is a list of words\n",
    "        # use as object[tokens]\n",
    "        # map token -> index -> vector\n",
    "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
    "                   for token in tokens]\n",
    "        vecs = self.idx_to_vec[np.asarray(indices)]\n",
    "        return vecs\n",
    "    \n",
    "    def __call__(self, tokens):\n",
    "        # Use as object(tokens)\n",
    "        return self.__getitem__(tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding = GloVeWordEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by noting that the embedding of a word does not depend on its context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50)\n"
     ]
    }
   ],
   "source": [
    "# To obtain the embeddings of words:\n",
    "sentence1 = 'I love data science'\n",
    "embeddings1 = glove_embedding[sentence1.split()]  # using the __getitem__ method\n",
    "# alternatively: \n",
    "# embeddings1 = glove_embedding(sentence1.split())  # using the __call__ method\n",
    "print(embeddings1.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 50)\n"
     ]
    }
   ],
   "source": [
    "sentence2 = 'As a kid, I always wanted to study mathematics and science'\n",
    "embeddings2 = glove_embedding[sentence2.split()]\n",
    "print(embeddings2.shape)  # (number of words, dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Compare the embeddings of both the time the word \"science\" appears\n",
    "e1 = embeddings1[-1]\n",
    "e2 = embeddings2[-1]\n",
    "print(np.linalg.norm(e1-e2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will look at the cosine similarity between word embeddings. \n",
    "Recall that the cosine similarity between two vectors $u, v \\in \\mathbb{R}^d$ is defined as \n",
    "$$\n",
    "    S_{\\cos}(u, v) = \\frac{\\langle u, v\\rangle}{\\|u\\|_2  \\, \\|v\\|_2} .\n",
    "$$\n",
    "\n",
    "For any pair of vectors, the cosine similarity is always \n",
    "between $-1$ and $1$. (Why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the k nearest neighbors \n",
    "# as per cosine similarity\n",
    "def k_nearest_neighbors(population, query, k):\n",
    "    # population is a matrix of size (n, dim)\n",
    "    # query is a matrix of shape (1, dim)\n",
    "    # k is an integer\n",
    "    # return topk indices and topk values\n",
    "    cos = np.dot(population, query.reshape(-1,)) / (\n",
    "        np.sqrt(np.sum(population * population, axis=1) + 1e-9) *\n",
    "        np.linalg.norm(query.reshape(-1))\n",
    "    )\n",
    "    topk_idx = np.argpartition(cos, -k)[-k:] # unsorted\n",
    "    topk_idx = topk_idx[np.argsort(cos[topk_idx])][::-1]  # sorted\n",
    "    topk_val = cos[topk_idx]\n",
    "    return topk_idx, topk_val\n",
    "\n",
    "# Hint for topk in numpy: https://stackoverflow.com/a/23734295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 microwave 0.9999999999839023\n",
      "1 analog 0.7300107691175304\n",
      "2 microwaves 0.7264979322621883\n",
      "3 oven 0.7115686481570181\n",
      "4 refrigerator 0.7039825402692077\n",
      "5 ovens 0.6948281242683831\n"
     ]
    }
   ],
   "source": [
    "query = glove_embedding(['microwave'])\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 6)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.idx_to_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 serendipity 0.9999999999485274\n",
      "1 profundity 0.7222786942557122\n",
      "2 happenstance 0.7016944428733708\n",
      "3 strangeness 0.6975966805924638\n",
      "4 weirdness 0.6901399894427984\n",
      "5 hokum 0.6882356254537078\n"
     ]
    }
   ],
   "source": [
    "query = glove_embedding(['serendipity'])\n",
    "topk_idxs, topk_vals = k_nearest_neighbors(glove_embedding.idx_to_vec, query, 6)\n",
    "topk_words = [glove_embedding.idx_to_token[i] for i in topk_idxs]\n",
    "\n",
    "for i, (w, val) in enumerate(zip(topk_words, topk_vals)):\n",
    "    print(i, w, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies with word embeddings\n",
    "\n",
    "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, “man”:“woman”::“son”:“daughter” is an example of analogy, “man” is to “woman” as “son” is to “daughter”. \n",
    "\n",
    "The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a:b::c:d$, given the first three words, a, b and c, we want to find d. \n",
    "\n",
    "Assume the word vector for the word w is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)−\\text{vec}(a)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(token_a, token_b, token_c):\n",
    "    # Implement the analogy from above\n",
    "    vecs = glove_embedding[[token_a, token_b, token_c]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    topk, cos = k_nearest_neighbors(glove_embedding.idx_to_vec, x, 1)\n",
    "    return glove_embedding.idx_to_token[int(topk[0])]  # Remove unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daughter'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_analogy('man', 'woman', 'son')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# capital-country\n",
    "get_analogy('london', 'england', 'paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'looked'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tense\n",
    "get_analogy('dance', 'danced', 'look')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection with Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run a test to compare two different models. This is called the McNemar's test. \n",
    "\n",
    "Let $h_1$ and $h_2$ be two different classification algorithms. \n",
    "The hypotheses we're testing are:\n",
    "$$\n",
    "H_0 : \\quad \\text{acc}(h_1) = \\text{acc}(h_2) \\\\\n",
    "H_1 : \\quad \\text{acc}(h_1) \\ne \\text{acc}(h_2) ,\n",
    "$$\n",
    "where acc$(h)$ is the accuracy of the classifier $h$.\n",
    "\n",
    "\n",
    "To distinguish between the two, we compute two the following numbers:\n",
    "- $N_{01}$: the number of validation examples misclassified by $h_1$ but correctly classified by $h_2$\n",
    "- $N_{10}$: the number of validation examples correctly classified by $h_1$ but misclassified by $h_2$. \n",
    "\n",
    "The test statistic is \n",
    "$$\n",
    "    T = \\frac{(|N_{01} - N_{10}| - 1)^2}{N_{10} + N_{01}}.\n",
    "$$\n",
    "Its asymptotic distribution under the null is $\\chi^2$-distribution with $1$ degree of freedom. We reject the test null hypothesis if \n",
    "$$T > \\chi^2_{1, \\alpha}$$\n",
    "the $(1-\\alpha)$-quantile of the $\\chi^2_1$ distribution.\n",
    "\n",
    "**The exercise**:\n",
    "Run this hypothesis test to compare the MLP from week 1 and the ConvNet from week 2. Use a significance $\\alpha=0.01$. Train each network with SGD for $30$ passes with an appropriate learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torch.nn.functional import cross_entropy\n",
    "import time\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d00eaa498df43dda6d63a20cdbb1ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f242084d729c44be8fcd4fc33273183d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417899795cd24fe5b5928e25c9eaefdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b370e811a284daa84ed25ee1787a120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
      "\n",
      "X_train.shape = torch.Size([6000, 28, 28])\n",
      "n_train: 6000, n_test: 10000\n",
      "Image size: torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# download dataset (~117M in size)\n",
    "train_dataset = FashionMNIST('./data', train=True, download=True)\n",
    "X_train = train_dataset.data # torch tensor of type uint8\n",
    "y_train = train_dataset.targets # torch tensor of type Long\n",
    "test_dataset = FashionMNIST('./data', train=False, download=True)\n",
    "X_test = test_dataset.data\n",
    "y_test = test_dataset.targets\n",
    "\n",
    "# choose a subsample of 10% of the data:\n",
    "idxs_train = torch.from_numpy(\n",
    "    np.random.choice(X_train.shape[0], replace=False, size=X_train.shape[0]//10))\n",
    "X_train, y_train = X_train[idxs_train], y_train[idxs_train]\n",
    "# idxs_test = torch.from_numpy(\n",
    "#     np.random.choice(X_test.shape[0], replace=False, size=X_test.shape[0]//10))\n",
    "# X_test, y_test = X_test[idxs_test], y_test[idxs_test]\n",
    "\n",
    "print(f'X_train.shape = {X_train.shape}')\n",
    "print(f'n_train: {X_train.shape[0]}, n_test: {X_test.shape[0]}')\n",
    "print(f'Image size: {X_train.shape[1:]}')\n",
    "\n",
    "# Normalize dataset: pixel values lie between 0 and 255\n",
    "# Normalize them so the pixelwise mean is zero and standard deviation is 1\n",
    "\n",
    "X_train = X_train.float()  # convert to float32\n",
    "X_train = X_train.view(-1, 784)\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean[None, :]) / (std[None, :] + 1e-6)  # avoid divide by zero\n",
    "\n",
    "X_test = X_test.float()\n",
    "X_test = X_test.view(-1, 784)\n",
    "X_test = (X_test - mean[None, :]) / (std[None, :] + 1e-6)\n",
    "\n",
    "n_class = np.unique(y_train).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules and SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class MLP(torch.nn.Module): \n",
    "    def __init__(self, hidden_width=32):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(784, hidden_width)\n",
    "        self.linear2 = torch.nn.Linear(32, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self,num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_ensemble_1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.conv_ensemble_2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        self.fc = torch.nn.Linear(7*7*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        out = self.conv_ensemble_1(x)\n",
    "        out = self.conv_ensemble_2(out)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "# Some utility functions to compute the objective and the accuracy\n",
    "def compute_objective(model, X, y):\n",
    "    score = model(X)\n",
    "    # PyTorch's function cross_entropy computes the multinomial logistic loss\n",
    "    return cross_entropy(input=score, target=y, reduction='mean') \n",
    "\n",
    "def sgd_one_pass(model, X, y, learning_rate, verbose=False):\n",
    "    num_examples = X.shape[0]\n",
    "    average_loss = 0.0\n",
    "    for i in range(num_examples):\n",
    "        idx = np.random.choice(X.shape[0])\n",
    "        # compute the objective. \n",
    "        # Note: This function requires X to be of shape (n,d). In this case, n=1 \n",
    "        objective = compute_objective(model, X[idx:idx+1], y[idx:idx+1]) \n",
    "        average_loss = 0.99 * average_loss + 0.01 * objective.item()\n",
    "        if verbose and (i+1) % 100 == 0:\n",
    "            print(average_loss)\n",
    "        \n",
    "        # compute the gradient using automatic differentiation\n",
    "        gradients = torch.autograd.grad(outputs=objective, inputs=model.parameters())\n",
    "        \n",
    "        # perform SGD update. IMPORTANT: Make the update inplace!\n",
    "        for (w, g) in zip(model.parameters(), gradients):\n",
    "            w.data -= learning_rate * g.data\n",
    "      \n",
    "    \n",
    "from tqdm.auto import trange # range + progress bar\n",
    "def sgd_n_passes(model, X_train, y_train, X_val, y_val, n_passes, learning_rate):\n",
    "    for i in trange(n_passes):\n",
    "        sgd_one_pass(model, X_train, y_train, learning_rate)\n",
    "    return compute_prediction_performance(model, X_val, y_val)\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_prediction_performance(model, X, y):\n",
    "    # return a boolean vector of the same length as y\n",
    "    # each entry is True if correctly predicted, else False\n",
    "    pred = torch.argmax(model(X), axis=1)\n",
    "    return y == pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8497f64350e645f78fb2b10867b20cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1 = MLP()\n",
    "performance1 = sgd_n_passes(\n",
    "    model1, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae7808b787e48788f09ee5a1e953cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model2 = ConvNet()\n",
    "performance2 = sgd_n_passes(\n",
    "    model2, X_train, y_train, X_test, y_test, n_passes=30, learning_rate=2.5e-3\n",
    ")\n",
    "# boolean vector of length n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of MLP: 0.8325\n",
      "accuracy of ConvNet: 0.8695\n"
     ]
    }
   ],
   "source": [
    "print('accuracy of MLP:', performance1.sum().item()/y_test.shape[0])\n",
    "print('accuracy of ConvNet:', performance2.sum().item()/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test whether this difference is statistically significant or not. From a \n",
    "first glance, it does appear to be statistically significant as the gap is quite large. \n",
    "\n",
    "Compute $N_{01}$ and $N_{10}$ from the output of SGD above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N01 = (~performance1 & performance2).sum().item()  # MLP is wrong but ConvNet is correct\n",
    "N10 = (performance1 & ~performance2).sum().item()  # MLP is correct but ConvNet is wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the test statistic, the threshold, which is the $(1-\\alpha)$ quantile of the $\\chi^2_1$ distribution and read off the conclusion of the test. Recall that we have $\\alpha=0.01$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test statistic: 118.60714285714286, threshold: 6.6348966010212145\n",
      "Null rejected\n"
     ]
    }
   ],
   "source": [
    "T = (abs(N01 - N10) - 1)**2 / (N01 + N10)  # statistic\n",
    "threshold = scipy.stats.chi2(df=1).ppf(0.99)\n",
    "\n",
    "print(f'Test statistic: {T}, threshold: {threshold}')\n",
    "\n",
    "if T > threshold:\n",
    "    print('Null rejected')\n",
    "else:\n",
    "    print('Failed to reject the null')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Sentiment Analysis using GloVe Embeddings\n",
    "\n",
    "The goal of this exercise is to repeat perform sentiment analysis using the data from the demo, except using GloVe embeddings. \n",
    "\n",
    "\n",
    "Download the data from [here](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data?select=train.tsv.zip).\n",
    "\n",
    "We will use movie reviews from Rotten Tomatoes. The sentiment labels are:\n",
    "- 0 - negative\n",
    "- 1 - somewhat negative\n",
    "- 2 - neutral\n",
    "- 3 - somewhat positive\n",
    "- 4 - positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SentenceId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This quiet , introspective and entertaining in...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Even fans of Ismail Merchant 's work , I suspe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A positively thrilling combination of ethnogra...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Phrase  Sentiment\n",
       "SentenceId                                                              \n",
       "1           A series of escapades demonstrating the adage ...          1\n",
       "2           This quiet , introspective and entertaining in...          4\n",
       "3           Even fans of Ismail Merchant 's work , I suspe...          1\n",
       "4           A positively thrilling combination of ethnogra...          3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "filename = './train.tsv'\n",
    "# keep one example per sentence (original data labels each phrase)\n",
    "data = pd.read_csv(filename, sep='\\t').groupby('SentenceId').first()\n",
    "data = data.drop(columns=['PhraseId'])\n",
    "\n",
    "\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split and featurize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 2) (1529, 2)\n"
     ]
    }
   ],
   "source": [
    "data = data.sample(frac=1)  # shuffle\n",
    "train_data = data[:7000]\n",
    "test_data = data[7000:]\n",
    "print(train_data.shape, test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\varphi(w)$ denote the embedding of word $w$.\n",
    "Recall that GloVe is a global embedding which does not depend on the context of the word. \n",
    "\n",
    "For a piece of text denoted by words $T = (w_1, \\cdots, w_n)$, we \n",
    "summarize it by the vector \n",
    "$$\n",
    "    \\psi(T) := \\frac{1}{n} \\sum_{i=1}^n w_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def featurize(x): # x is pd.Series with text\n",
    "    features = []\n",
    "    for sen in tqdm(x):\n",
    "        sen = tokenizer.encode(sen, return_tensors='pt')\n",
    "        outputs = model(sen, return_dict=True)\n",
    "        embeddings = outputs.last_hidden_state.squeeze() # (len, dim)\n",
    "        mean_embedding = embeddings.mean(axis=0)\n",
    "        features.append(mean_embedding.numpy())\n",
    "    return np.stack(features)  # (n, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc3a860db754376a889043ea7384ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd212da0f81470c8c2d62e856258f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "# Download the pre-trained model + tokenizer (a total of 440 MB)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name) # to tokenize the text\n",
    "model = BertModel.from_pretrained(model_name)  # PyTorch module\n",
    "\n",
    "x_train = featurize(train_data['Phrase'])\n",
    "y_train = train_data['Sentiment'].values\n",
    "\n",
    "x_test = featurize(test_data['Phrase'])\n",
    "y_test = test_data['Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a simple logistic regression classifier to test performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will reduce the data dimensionality\n",
    "# This step is optional and only perform it if you \n",
    "# find that it helps the test accuracy\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, random_state=1).fit(x_train)  # keep 99% of the explained variance\n",
    "x_train = pca.transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C after cross-validation: 1.02940790749159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "# TODO: Tune C with cross-validation\n",
    "clf = LogisticRegressionCV(random_state=0, Cs=np.logspace(1e-4, 1e-1, 25), cv=5, max_iter=200).fit(x_train, y_train)\n",
    "print(f'Best C after cross-validation: {clf.C_[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.6228571428571429\n",
      "Test accuracy: 0.43361674296926095\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = clf.predict(x_train)\n",
    "y_test_pred = clf.predict(x_test)\n",
    "\n",
    "print('Train accuracy:', (y_train_pred == y_train).mean())\n",
    "print('Test accuracy:', (y_test_pred == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sentence):\n",
    "    sentence = pd.Series([sentence])\n",
    "    sentence = featurize(sentence)\n",
    "    sentence = pca.transform(sentence)\n",
    "    print(f'The model predicts: {clf.predict(sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c5f2d577c4af5a38563aa19f93c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts: [2]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d697317c3e455cb77110d208eaa97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts: [4]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ee69033a544bdf88cb271b80acbcea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model predicts: [1]\n"
     ]
    }
   ],
   "source": [
    "# Test random sentences\n",
    "test_sentence('This movie is the best as sucking')\n",
    "test_sentence('I think this is awesome')\n",
    "test_sentence('The Room exceeds the concepts of good movie-making')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data598]",
   "language": "python",
   "name": "conda-env-data598-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
